{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Aprendizaje reforzado\n", "\n", "Este cuaderno de Jupyter act\u00faa como material de apoyo para el **Cap\u00edtulo 21 Aprendizaje por refuerzo** del libro* Inteligencia artificial: un enfoque moderno*. Este cuaderno utiliza las implementaciones del m\u00f3dulo `rl.py`. Tambi\u00e9n utilizamos la implementaci\u00f3n de MDP en el m\u00f3dulo `mdp.py` para probar nuestros agentes. Puede resultar \u00fatil si ya ha le\u00eddo el cuaderno de Jupyter que trata sobre el proceso de decisi\u00f3n de Markov. Importemos todo desde el m\u00f3dulo `rl`. Podr\u00eda resultar \u00fatil ver el c\u00f3digo fuente de algunas de nuestras implementaciones. Consulte el cuaderno introductorio de Jupyter para obtener m\u00e1s detalles."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from reinforcement_learning import *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## CONTENIDO\n", "\n", "* Descripci\u00f3n general\n", "* Aprendizaje pasivo por refuerzo\n", "- Estimaci\u00f3n de Utilidad Directa\n", "- Programaci\u00f3n din\u00e1mica adaptativa\n", "- Agente de diferencia temporal\n", "*Aprendizaje por refuerzo activo\n", "- Q aprendizaje"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## DESCRIPCI\u00d3N GENERAL\n", "\n", "Antes de comenzar a jugar con las implementaciones reales, revisemos un par de cosas sobre RL.\n", "\n", "1. El aprendizaje por refuerzo se ocupa de c\u00f3mo los agentes de software deber\u00edan tomar acciones en un entorno para maximizar alguna noci\u00f3n de recompensa acumulativa.\n", "\n", "2. El aprendizaje por refuerzo se diferencia del aprendizaje supervisado est\u00e1ndar en que nunca se presentan los pares de entrada/salida correctos, ni se corrigen expl\u00edcitamente las acciones sub\u00f3ptimas. Adem\u00e1s, hay un enfoque en el desempe\u00f1o en l\u00ednea, que implica encontrar un equilibrio entre la exploraci\u00f3n (de territorio inexplorado) y la explotaci\u00f3n (del conocimiento actual).\n", "\n", "-- Fuente: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n", "\n", "En resumen, tenemos una secuencia de transiciones de acciones estatales con recompensas asociadas con algunos estados. Nuestro objetivo es encontrar la pol\u00edtica \u00f3ptima $\\pi$ que nos diga qu\u00e9 acci\u00f3n tomar en cada estado."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## APRENDIZAJE PASIVO POR REFUERZO\n", "\n", "En el aprendizaje por refuerzo pasivo, el agente sigue una pol\u00edtica fija $\\pi$. El aprendizaje pasivo intenta evaluar la pol\u00edtica dada $pi$, sin ning\u00fan conocimiento de la funci\u00f3n de recompensa $R(s)$ y el modelo de transici\u00f3n $P(s'\\ |\\ s, a)$.\n", "\n", "Esto generalmente se hace mediante alg\u00fan m\u00e9todo de **estimaci\u00f3n de utilidad**. El agente intenta aprender directamente la utilidad de cada estado que resultar\u00eda de seguir la pol\u00edtica. Tenga en cuenta que en cada paso, tiene que *percibir* la recompensa y el estado; no tiene conocimiento global de estos. Por lo tanto, si un conjunto completo de acciones ofrece una probabilidad muy baja de alcanzar alg\u00fan estado $s_+$, es posible que el agente nunca perciba la recompensa $R(s_+)$.\n", "\n", "Considere una situaci\u00f3n en la que a un agente se le da una pol\u00edtica a seguir. Por lo tanto, en cualquier momento s\u00f3lo conoce su estado actual y su recompensa actual, y la acci\u00f3n que debe realizar a continuaci\u00f3n. Esta acci\u00f3n puede llevarlo a m\u00e1s de un estado, con diferentes probabilidades.\n", "\n", "Para una serie de acciones dadas por $\\pi$, la utilidad estimada $U$:\n", "$$U^{\\pi}(s) = E(\\sum_{t=0}^\\inf \\gamma^t R^t(s')$$)\n", "O el valor esperado de la suma de las recompensas descontadas hasta la terminaci\u00f3n.\n", "\n", "Con base en este concepto, analizamos tres m\u00e9todos para estimar la utilidad:\n", "\n", "1. **Estimaci\u00f3n directa de utilidad (DUE)**\n", "\n", "El primer m\u00e9todo, el m\u00e1s ingenuo, de estimar la utilidad proviene de la interpretaci\u00f3n m\u00e1s simple de la definici\u00f3n anterior. Construimos un agente que sigue la pol\u00edtica hasta llegar al estado terminal. En cada paso, registra su estado actual, recompensa. Una vez que alcanza el estado terminal, puede estimar la utilidad de cada estado para *esa* iteraci\u00f3n, simplemente sumando las recompensas descontadas de ese estado al estado terminal.\n", "\n", "Ahora puede ejecutar esta 'simulaci\u00f3n' $n$ veces y calcular la utilidad promedio de cada estado. Si un estado ocurre m\u00e1s de una vez en una simulaci\u00f3n, ambos valores de utilidad se cuentan por separado.\n", "\n", "Tenga en cuenta que este m\u00e9todo puede resultar prohibitivamente lento para espacios de estados muy grandes. Adem\u00e1s, **no presta atenci\u00f3n a la probabilidad de transici\u00f3n $P(s'\\ |\\ s, a)$.** Se pierde informaci\u00f3n que es capaz de recopilar (digamos, registrando el n\u00famero de veces que se realiza una acci\u00f3n). de un estado llev\u00f3 a otro estado). El siguiente m\u00e9todo aborda este problema.\n", "\n", "2. **Programaci\u00f3n din\u00e1mica adaptativa (ADP)**\n", "\n", "Este m\u00e9todo hace uso del conocimiento del estado pasado $s$, la acci\u00f3n $a$ y el nuevo estado percibido $s'$ para estimar la probabilidad de transici\u00f3n $P(s'\\ |\\ s,a)$. Lo hace mediante el simple recuento de nuevos estados resultantes de estados y acciones anteriores.<br>\n", "El programa ejecuta la pol\u00edtica varias veces, realizando un seguimiento de:\n", "- cada aparici\u00f3n del estado $s$ y la acci\u00f3n recomendada por la pol\u00edtica $a$ en $N_{sa}$\n", "- cada aparici\u00f3n de $s'$ resultante de $a$ en $s$ en $N_{s'|sa}$.\n", "\n", "Por lo tanto, puede estimar $P(s'\\ |\\ s,a)$ como $N_{s'|sa}/N_{sa}$, que en el l\u00edmite de infinitas pruebas, converger\u00e1 al valor verdadero.<br >\n", "Utilizando las probabilidades de transici\u00f3n as\u00ed estimadas, se puede aplicar la \"EVALUACI\u00d3N DE POL\u00cdTICAS\" para estimar las utilidades $U(s)$ utilizando propiedades de convergencia de las funciones de Bellman.\n", "\n", "3. **Aprendizaje de diferencia temporal (TD)**\n", "\n", "En lugar de construir expl\u00edcitamente el modelo de transici\u00f3n $P$, el modelo de diferencia temporal hace uso de la cercan\u00eda esperada entre las utilidades de dos estados consecutivos $s$ y $s'$.\n", "Para la transici\u00f3n $s$ a $s'$, la actualizaci\u00f3n se escribe como:\n", "$$U^{\\pi}(s) \\leftarrow U^{\\pi}(s) + \\alpha \\left( R(s) + \\gamma U^{\\pi}(s') - U^{\\ pi}(s) \\derecha)$$\n", "Este modelo incorpora impl\u00edcitamente las probabilidades de transici\u00f3n al sopesar cada estado por el n\u00famero de veces que se alcanza desde el estado actual. Por tanto, tras varias iteraciones, converge de manera similar a las ecuaciones de Bellman.\n", "La ventaja del modelo de aprendizaje TD es su c\u00e1lculo relativamente simple en cada paso, en lugar de tener que realizar un seguimiento de varios conteos.\n", "Para $n_s$ estados y $n_a$ acciones, el modelo ADP tendr\u00eda $n_s \\times n_a$ n\u00fameros $N_{sa}$ y $n_s^2 \\times n_a$ n\u00fameros $N_{s'|sa}$ para realizar un seguimiento de. El modelo TD solo debe realizar un seguimiento de una utilidad $U(s)$ para cada estado."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Demostrando agentes pasivos\n", "\n", "Los agentes pasivos se implementan en `rl.py` como varias `Clase de agente`.\n", "\n", "Para demostrar estos agentes, utilizamos el objeto `GridMDP` del m\u00f3dulo `MDP`. `sequential_decision_environment` es similar al usado para el port\u00e1til `MDP` pero tiene un descuento de $\\gamma = 0,9$.\n", "\n", "El \"Programa-Agente\" se puede obtener creando una instancia de la \"Clase-Agente\" correspondiente. El m\u00e9todo `__call__` permite llamar a la `Clase-Agente` como una funci\u00f3n. Es necesario crear una instancia de la clase con una pol\u00edtica ($\\pi$) y un \"MDP\" cuya utilidad de los estados se estimar\u00e1."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from mdp import sequential_decision_environment"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El `sequential_decision_environment` es un objeto GridMDP como se muestra a continuaci\u00f3n. Las recompensas son **+1** y **-1** en los estados terminales, y **-0,04** en el resto. <img src=\"files/images/mdp.png\"> Ahora definimos acciones y una pol\u00edtica similar a **Fig 21.1** en el libro."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Action Directions\n", "north = (0, 1)\n", "south = (0,-1)\n", "west = (-1, 0)\n", "east = (1, 0)\n", "\n", "policy = {\n", "    (0, 2): east,  (1, 2): east,  (2, 2): east,   (3, 2): None,\n", "    (0, 1): north,                (2, 1): north,  (3, 1): None,\n", "    (0, 0): north, (1, 0): west,  (2, 0): west,   (3, 0): west, \n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Agente de estimaci\u00f3n de servicios p\u00fablicos de direcci\u00f3n\n", "\n", "La clase `PassiveDEUAgent` en el m\u00f3dulo `rl` implementa el Programa Agente descrito en **Fig 21.2** del Libro AIMA. `PassiveDEUAgent` suma las recompensas para encontrar la utilidad estimada para cada estado. Por tanto, requiere la ejecuci\u00f3n de varias iteraciones."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%psource PassiveDUEAgent"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["DUEagent = PassiveDUEAgent(policy, sequential_decision_environment)\n", "for i in range(200):\n", "    run_single_trial(DUEagent, sequential_decision_environment)\n", "    DUEagent.estimate_U()\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Las utilidades calculadas son:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('\\n'.join([str(k)+':'+str(v) for k, v in DUEagent.U.items()]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Agente de programaci\u00f3n din\u00e1mica adaptativa\n", "\n", "La clase `PassiveADPAgent` en el m\u00f3dulo `rl` implementa el Programa Agente descrito en **Fig 21.2** del Libro AIMA. `PassiveADPAgent` utiliza transici\u00f3n de estado y recuentos de ocurrencias para estimar $P$ y luego $U$. Consulte la fuente a continuaci\u00f3n para comprender al agente."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%psource PassiveADPAgent"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creamos una instancia de un `PassiveADPAgent` a continuaci\u00f3n con el `GridMDP` que se muestra y lo entrenamos en m\u00e1s de 200 iteraciones. El m\u00f3dulo `rl` tiene una implementaci\u00f3n simple para simular iteraciones. La funci\u00f3n se llama **run_single_trial**."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["ADPagent = PassiveADPAgent(policy, sequential_decision_environment)\n", "for i in range(200):\n", "    run_single_trial(ADPagent, sequential_decision_environment)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Las utilidades calculadas son:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["print('\\n'.join([str(k)+':'+str(v) for k, v in ADPagent.U.items()]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Agente pasivo de diferencia temporal\n", "\n", "\"PassiveTDAgent\" utiliza diferencias temporales para conocer estimaciones de utilidad. Aprendemos la diferencia entre los estados y respaldamos los valores en estados anteriores. Analicemos la fuente antes de ver algunos ejemplos de uso."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%psource PassiveTDAgent"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Al crear el `TDAgent`, utilizamos la **misma tasa de aprendizaje** $\\alpha$ que se indica en la nota al pie del libro en la **p\u00e1gina 837**."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["TDagent = PassiveTDAgent(policy, sequential_decision_environment, alpha = lambda n: 60./(59+n))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora realizamos **200 pruebas** para que el agente estime las utilidades."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["for i in range(200):\n", "    run_single_trial(TDagent,sequential_decision_environment)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Las utilidades calculadas son:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('\\n'.join([str(k)+':'+str(v) for k, v in TDagent.U.items()]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparaci\u00f3n con el m\u00e9todo de iteraci\u00f3n de valores.\n", "\n", "Tambi\u00e9n podemos comparar las estimaciones de utilidad aprendidas por nuestro agente con las obtenidas mediante **iteraci\u00f3n de valor**.\n", "\n", "**Tenga en cuenta que la iteraci\u00f3n de valor tiene conocimiento a priori de la tabla de transici\u00f3n $P$, las recompensas $R$ y todos los estados $s$.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from mdp import value_iteration"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Los valores calculados por iteraci\u00f3n de valores:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["U_values = value_iteration(sequential_decision_environment)\n", "print('\\n'.join([str(k)+':'+str(v) for k, v in U_values.items()]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Evoluci\u00f3n de las estimaciones de utilidad a lo largo de iteraciones\n", "\n", "Podemos explorar c\u00f3mo var\u00edan estas estimaciones con el tiempo utilizando gr\u00e1ficos similares a **Figura 21.5a**. Primero habilitaremos matplotlib usando el backend en l\u00ednea. Tambi\u00e9n definimos una funci\u00f3n para recopilar los valores de las utilidades en cada iteraci\u00f3n."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "def graph_utility_estimates(agent_program, mdp, no_of_iterations, states_to_graph):\n", "    graphs = {state:[] for state in states_to_graph}\n", "    for iteration in range(1,no_of_iterations+1):\n", "        run_single_trial(agent_program, mdp)\n", "        for state in states_to_graph:\n", "            graphs[state].append((iteration, agent_program.U[state]))\n", "    for state, value in graphs.items():\n", "        state_x, state_y = zip(*value)\n", "        plt.plot(state_x, state_y, label=str(state))\n", "    plt.ylim([0,1.2])\n", "    plt.legend(loc='lower right')\n", "    plt.xlabel('Iterations')\n", "    plt.ylabel('U')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aqu\u00ed hay un gr\u00e1fico del estado $(2,2)$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agent = PassiveTDAgent(policy, sequential_decision_environment, alpha=lambda n: 60./(59+n))\n", "graph_utility_estimates(agent, sequential_decision_environment, 500, [(2,2)])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tambi\u00e9n es posible trazar varios estados en el mismo gr\u00e1fico. Como se esperaba, la utilidad del estado finito $(3,2)$ permanece constante y es igual a $R((3,2)) = 1$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["graph_utility_estimates(agent, sequential_decision_environment, 500, [(2,2), (3,2)])"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["##APRENDIZAJE ACTIVO POR REFUERZO\n", "\n", "A diferencia del aprendizaje por refuerzo pasivo, en el aprendizaje por refuerzo activo no estamos sujetos a una pol\u00edtica pi y debemos seleccionar nuestras acciones. En otras palabras, el agente necesita aprender una pol\u00edtica \u00f3ptima. El equilibrio fundamental que el agente debe afrontar es el de exploraci\u00f3n versus explotaci\u00f3n."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### QAgente de aprendizaje\n", "\n", "La clase QLearningAgent en el m\u00f3dulo rl implementa el Programa Agente descrito en **Fig 21.8** del Libro AIMA. En Q-Learning, el agente aprende una funci\u00f3n de valor de acci\u00f3n Q que le brinda la utilidad de realizar una acci\u00f3n determinada en un estado particular. Q-Learning no requiere un modelo de transici\u00f3n y, por lo tanto, es un m\u00e9todo sin modelos. Analicemos la fuente antes de ver algunos ejemplos de uso."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%psource QLearningAgent"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El Programa Agente se puede obtener creando la instancia de la clase pasando los par\u00e1metros apropiados. Debido al m\u00e9todo __ call __, el objeto que se crea se comporta como un invocable y devuelve una acci\u00f3n apropiada como lo hacen la mayor\u00eda de los programas de agentes. Para crear una instancia del objeto necesitamos un mdp similar al PassiveTDAgent.\n", "\n", "Usemos el mismo objeto GridMDP que usamos anteriormente. **La Figura 17.1 (entorno_de decisi\u00f3n_sequential)** es similar a la **Figura 21.1** pero tiene algunos descuentos como **gamma = 0,9**. La clase tambi\u00e9n implementa una funci\u00f3n de exploraci\u00f3n **f** que devuelve **Rplus** fijo hasta que el agente haya visitado el estado, acci\u00f3n **Ne** n\u00famero de veces. Este es el mismo que se define en la p\u00e1gina **842** del libro. El m\u00e9todo **actions_in_state** devuelve acciones posibles en un estado determinado. Es \u00fatil al aplicar operaciones max y argmax."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creemos nuestro objeto ahora. Tambi\u00e9n utilizamos el **mismo alfa** que figura en la nota al pie del libro en la **p\u00e1gina 837**. Usamos **Rplus = 2** y **Ne = 5** como se define en la p\u00e1gina 843. **Fig 21.7**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["q_agent = QLearningAgent(sequential_decision_environment, Ne=5, Rplus=2, \n", "                         alpha=lambda n: 60./(59+n))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora, para probar q_agent utilizamos la funci\u00f3n **run_single_trial** en rl.py (que tambi\u00e9n se us\u00f3 anteriormente). Usemos **200** iteraciones."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["for i in range(200):\n", "    run_single_trial(q_agent,sequential_decision_environment)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora veamos los valores Q. Las claves son pares estado-acci\u00f3n. Donde corresponden diferentes acciones seg\u00fan:\n", "\n", "norte = (0, 1)\n", "sur = (0,-1)\n", "oeste = (-1, 0)\n", "este = (1, 0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["q_agent.Q"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La Utilidad **U** de cada estado est\u00e1 relacionada con **Q** mediante la siguiente ecuaci\u00f3n.\n", "\n", "**U(s) = m\u00e1x <sub>a</sub> Q(s, a)**\n", "\n", "Convirtamos los valores Q anteriores en estimaciones U.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["U = defaultdict(lambda: -1000.) # Very Large Negative Value for Comparison see below.\n", "for state_action, value in q_agent.Q.items():\n", "    state, action = state_action\n", "    if U[state] < value:\n", "                U[state] = value"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["U"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finalmente comparemos estas estimaciones con los resultados de value_iteration."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(value_iteration(sequential_decision_environment))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}, "pycharm": {"stem_cell": {"cell_type": "raw", "source": [], "metadata": {"collapsed": false}}}}, "nbformat": 4, "nbformat_minor": 1}