{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# TEXTO\n", "\n", "Este cuaderno sirve como material de apoyo para los temas tratados en el **Cap\u00edtulo 22: Procesamiento del lenguaje natural** del libro *Inteligencia artificial: un enfoque moderno*. Este cuaderno utiliza implementaciones de [text.py](https://github.com/aimacode/aima-python/blob/master/text.py)."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": true}, "outputs": [], "source": ["from text import *\n", "from utils import open_data\n", "from notebook import psource"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## CONTENIDO\n", "\n", "* Modelos de texto\n", "* Segmentaci\u00f3n de texto de Viterbi\n", "* Recuperaci\u00f3n de informaci\u00f3n\n", "* Extracci\u00f3n de informaci\u00f3n\n", "* Decodificadores"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## MODELOS DE TEXTO\n", "\n", "Antes de comenzar a analizar algoritmos de procesamiento de texto, necesitaremos crear algunos modelos de lenguaje. Esos modelos sirven como una tabla de b\u00fasqueda de probabilidades de caracteres o palabras (seg\u00fan el tipo de modelo). Estos modelos pueden darnos las probabilidades de que aparezcan palabras o secuencias de caracteres en el texto. Tomemos como ejemplo \"el\". Los modelos de texto pueden darnos la probabilidad de \"el\", *P(\"el\")*, ya sea como una palabra o como una secuencia de caracteres (\"t\" seguida de \"h\" seguida de \"e\"). La primera representaci\u00f3n se llama \"modelo de palabras\" y trata de palabras como objetos distintos, mientras que la segunda es un \"modelo de caracteres\" y trata de secuencias de caracteres como objetos. Tenga en cuenta que podemos especificar el n\u00famero de palabras o la longitud de las secuencias de caracteres para satisfacer mejor nuestras necesidades. Entonces, dado que el n\u00famero de palabras es igual a 2, tenemos probabilidades en la forma *P(palabra1, palabra2)*. Por ejemplo, *P(\"de\", \"el\")*. Para los modelos char, hacemos lo mismo pero para los chars.\n", "\n", "Tambi\u00e9n es \u00fatil almacenar las probabilidades condicionales de palabras dadas las palabras anteriores. Eso significa que, dado que encontramos las palabras \"de\" y \"the\", \u00bfcu\u00e1l es la probabilidad de que la siguiente palabra sea \"mundo\"? M\u00e1s formalmente, *P(\"mundo\"|\"de\", \"el\")*. Generalizando, *P(Wi|Wi-1, Wi-2, ... , Wi-n)*.\n", "\n", "Llamamos al modelo de palabras *Modelo de palabras N-Gram* (del griego \"gram\", la ra\u00edz de \"escribir\" o la palabra para \"letra\") y al modelo de caracteres *Modelo de caracteres N-Gram*. En el caso especial donde *N* es 1, llamamos a los modelos *Modelo de palabras de Unigram* y *Modelo de caracteres de Unigram* respectivamente.\n", "\n", "En el m\u00f3dulo `text` implementamos los dos modelos (tanto sus variantes de unigrama como de n-grama) heredando de `CountingProbDist` de `learning.py`. Tenga en cuenta que \"CountingProbDist\" no devuelve la probabilidad real de cada objeto, sino el n\u00famero de veces que aparece en nuestros datos de prueba.\n", "\n", "Para modelos de palabras tenemos `UnigramWordModel` y `NgramWordModel`. Les proporcionamos un archivo de texto y les muestran la frecuencia de las diferentes palabras. Tenemos `UnigramCharModel` y `NgramCharModel` para los modelos de personajes.\n", "\n", "Ejecute las celdas siguientes para ver el c\u00f3digo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(UnigramWordModel, NgramWordModel, UnigramCharModel, NgramCharModel)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A continuaci\u00f3n construimos nuestros modelos. El archivo de texto que usaremos para construirlos es *Flatland*, de Edwin A. Abbott. Lo cargaremos desde [here](https://github.com/aimacode/aima-data/blob/a21fc108f52ad551344e947b0eb97df82f8d2b2b/EN-text/flatland.txt). En ese directorio puede encontrar otros archivos de texto que podr\u00edamos usar aqu\u00ed."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Obteniendo probabilidades\n", "\n", "Aqu\u00ed veremos c\u00f3mo leer texto y encontrar las probabilidades de cada modelo, y c\u00f3mo recuperarlas.\n", "\n", "Primero la palabra modelos:"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[(2081, 'the'), (1479, 'of'), (1021, 'and'), (1008, 'to'), (850, 'a')]\n", "[(368, ('of', 'the')), (152, ('to', 'the')), (152, ('in', 'the')), (86, ('of', 'a')), (80, ('it', 'is'))]\n", "0.0036724740723330495\n", "0.00114584557527324\n"]}], "source": ["flatland = open_data(\"EN-text/flatland.txt\").read()\n", "wordseq = words(flatland)\n", "\n", "P1 = UnigramWordModel(wordseq)\n", "P2 = NgramWordModel(2, wordseq)\n", "\n", "print(P1.top(5))\n", "print(P2.top(5))\n", "\n", "print(P1['an'])\n", "print(P2[('i', 'was')])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vemos que la palabra m\u00e1s utilizada en *Planilandia* es 'the', con 2081 apariciones, mientras que la secuencia m\u00e1s utilizada es 'of the' con 368 apariciones. Adem\u00e1s, la probabilidad de 'un' es aproximadamente 0,003, mientras que para 'yo era' es cercana a 0,001. Tenga en cuenta que las cadenas utilizadas como claves est\u00e1n todas en min\u00fasculas. Para el modelo de unigrama, las claves son cadenas simples, mientras que para los modelos de n-gramas tenemos n-tuplas de cadenas.\n", "\n", "A continuaci\u00f3n, veremos c\u00f3mo podemos obtener informaci\u00f3n de las probabilidades condicionales del modelo y c\u00f3mo podemos generar la siguiente palabra en una secuencia."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Conditional Probabilities Table: {'now': 2, 'glad': 1, 'keenly': 1, 'considered': 1, 'once': 2, 'not': 4, 'in': 2, 'by': 1, 'simulating': 1, 'intoxicated': 1, 'wearied': 1, 'quite': 1, 'certain': 2, 'sitting': 1, 'to': 2, 'rapidly': 1, 'will': 1, 'describing': 1, 'allowed': 1, 'at': 2, 'afraid': 1, 'covered': 1, 'approaching': 1, 'standing': 1, 'myself': 1, 'surprised': 1, 'unusually': 1, 'rapt': 1, 'pleased': 1, 'crushed': 1} \n", "\n", "Conditional Probability of 'once' give 'i was': 0.05128205128205128 \n", "\n", "Next word after 'i was': wearied\n"]}], "source": ["flatland = open_data(\"EN-text/flatland.txt\").read()\n", "wordseq = words(flatland)\n", "\n", "P3 = NgramWordModel(3, wordseq)\n", "\n", "print(\"Conditional Probabilities Table:\", P3.cond_prob[('i', 'was')].dictionary, '\\n')\n", "print(\"Conditional Probability of 'once' give 'i was':\", P3.cond_prob[('i', 'was')]['once'], '\\n')\n", "print(\"Next word after 'i was':\", P3.cond_prob[('i', 'was')].sample())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Primero imprimimos todas las palabras posibles que vienen despu\u00e9s de 'yo era' y las veces que han aparecido en el modelo. A continuaci\u00f3n, imprimimos la probabilidad de que aparezca 'una vez' despu\u00e9s de 'yo era' y, finalmente, elegimos una palabra para continuar despu\u00e9s de 'yo era'. Tenga en cuenta que la palabra se elige seg\u00fan su probabilidad de aparecer (un recuento alto de apariciones significa una mayor probabilidad de ser elegida)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Echemos un vistazo a los dos modelos de personajes:"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[(19208, 'e'), (13965, 't'), (12069, 'o'), (11702, 'a'), (11440, 'i')]\n", "[(5364, (' ', 't')), (4573, ('t', 'h')), (4063, (' ', 'a')), (3654, ('h', 'e')), (2967, (' ', 'i'))]\n", "0.0006028715031814578\n", "0.0032371578540395666\n"]}], "source": ["flatland = open_data(\"EN-text/flatland.txt\").read()\n", "wordseq = words(flatland)\n", "\n", "P1 = UnigramCharModel(wordseq)\n", "P2 = NgramCharModel(2, wordseq)\n", "\n", "print(P1.top(5))\n", "print(P2.top(5))\n", "\n", "print(P1['z'])\n", "print(P2[('g', 'h')])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La letra m\u00e1s com\u00fan es 'e', \u200b\u200baparece m\u00e1s de 19000 veces y la secuencia m\u00e1s com\u00fan es \"\\_t\". Es decir, un espacio seguido de una 't'. Tenga en cuenta que aunque no contamos los espacios para los modelos de palabras o los modelos de caracteres unigramas, s\u00ed los contamos para los modelos de caracteres de n-gramas.\n", "\n", "Adem\u00e1s, la probabilidad de que aparezca la letra 'z' es cercana a 0,0006, mientras que para el bigrama 'gh' es de 0,003."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Generando muestras\n", "\n", "Adem\u00e1s de leer las probabilidades de n-gramas, tambi\u00e9n podemos usar nuestro modelo para generar secuencias de palabras, usando la funci\u00f3n \"muestras\" en los modelos de palabras."]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["hearing as inside is confined to conduct by the duties\n", "all and of voice being in a day of the\n", "party they are stirred to mutual warfare and perish by\n"]}], "source": ["flatland = open_data(\"EN-text/flatland.txt\").read()\n", "wordseq = words(flatland)\n", "\n", "P1 = UnigramWordModel(wordseq)\n", "P2 = NgramWordModel(2, wordseq)\n", "P3 = NgramWordModel(3, wordseq)\n", "\n", "print(P1.samples(10))\n", "print(P2.samples(10))\n", "print(P3.samples(10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["En el modelo de unigrama, la mayor\u00eda de las veces obtenemos galimat\u00edas, ya que cada palabra se selecciona seg\u00fan su frecuencia de aparici\u00f3n en el texto, sin tener en cuenta las palabras anteriores. Sin embargo, a medida que aumentamos *n*, comenzamos a obtener muestras que tienen cierta apariencia de coherencia y recuerdan un poco al ingl\u00e9s normal. A medida que aumentemos nuestros datos, estas muestras mejorar\u00e1n.\n", "\n", "Vamos a intentarlo. Agregaremos al modelo m\u00e1s datos con los que trabajar y veremos qu\u00e9 sale."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["leave them at cleveland this christmas now pray do not ask you to relate or\n", "meaning and both of us sprang forward in the direction and no sooner had they\n", "palmer though very unwilling to go as well from real humanity and good nature as\n", "time about what they should do and they agreed he should take orders directly and\n"]}], "source": ["data = open_data(\"EN-text/flatland.txt\").read()\n", "data += open_data(\"EN-text/sense.txt\").read()\n", "\n", "wordseq = words(data)\n", "\n", "P3 = NgramWordModel(3, wordseq)\n", "P4 = NgramWordModel(4, wordseq)\n", "P5 = NgramWordModel(5, wordseq)\n", "P7 = NgramWordModel(7, wordseq)\n", "\n", "print(P3.samples(15))\n", "print(P4.samples(15))\n", "print(P5.samples(15))\n", "print(P7.samples(15))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Observe c\u00f3mo las muestras comienzan a volverse cada vez m\u00e1s razonables a medida que agregamos m\u00e1s datos y aumentamos el par\u00e1metro *n*. Todav\u00eda estamos muy lejos de la generaci\u00f3n de texto realista, pero al mismo tiempo podemos ver que con suficientes datos incluso los algoritmos rudimentarios pueden producir algo casi aceptable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## SEGMENTACI\u00d3N DE TEXTO DE VITERBI\n", "\n", "### Descripci\u00f3n general\n", "\n", "Nos dan una cadena que contiene palabras de una oraci\u00f3n, \u00a1pero todos los espacios han desaparecido! Es muy dif\u00edcil de leer y nos gustar\u00eda separar las palabras de la cadena. Podemos lograr esto empleando el algoritmo de \"Segmentaci\u00f3n de Viterbi\". Toma como entrada la cadena a segmentar y un modelo de texto, y devuelve una lista de palabras separadas.\n", "\n", "El algoritmo opera en un enfoque de programaci\u00f3n din\u00e1mica. Comienza desde el principio de la cadena y construye iterativamente la mejor soluci\u00f3n utilizando soluciones anteriores. Lo logra segmentando la cadena en \"ventanas\", cada ventana representa una palabra (real o galimat\u00edas). Luego calcula la probabilidad de que ocurra la secuencia hasta esa ventana/palabra y actualiza su soluci\u00f3n. Cuando termina, rastrea desde la \u00faltima palabra y encuentra la secuencia completa de palabras."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implementaci\u00f3n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(viterbi_segment)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La funci\u00f3n toma como entrada una cadena y un modelo de texto, y devuelve la secuencia de palabras m\u00e1s probable, junto con la probabilidad de esa secuencia.\n", "\n", "La \"ventana\" es `w` e incluye los caracteres desde *j* hasta *i*. Lo usamos para \"construir\" la siguiente secuencia: desde el principio hasta *j* y luego `w`. Anteriormente hemos calculado la probabilidad desde el inicio hasta *j*, as\u00ed que ahora multiplicamos esa probabilidad por `P[w]` to get the probability of the whole sequence. If that probability is greater than the probability we have calculated so far for the sequence from the start to *i* (`best[i]`), la actualizamos."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Ejemplo\n", "\n", "El modelo que utiliza el algoritmo es `UnigramTextModel`. Primero construiremos el modelo usando el texto *Flatland* y luego intentaremos separar una oraci\u00f3n sin espacios."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Sequence of words is: ['it', 'is', 'easy', 'to', 'read', 'words', 'without', 'spaces']\n", "Probability of sequence is: 2.273672843573388e-24\n"]}], "source": ["flatland = open_data(\"EN-text/flatland.txt\").read()\n", "wordseq = words(flatland)\n", "P = UnigramWordModel(wordseq)\n", "text = \"itiseasytoreadwordswithoutspaces\"\n", "\n", "s, p = viterbi_segment(text,P)\n", "print(\"Sequence of words is:\",s)\n", "print(\"Probability of sequence is:\",p)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El algoritmo recuper\u00f3 correctamente las palabras de la cadena. Tambi\u00e9n nos dio la probabilidad de esta secuencia, que es peque\u00f1a, pero sigue siendo la segmentaci\u00f3n m\u00e1s probable de la cuerda."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## RECUPERACI\u00d3N DE INFORMACI\u00d3N\n", "\n", "### Descripci\u00f3n general\n", "\n", "Con **Recuperaci\u00f3n de informaci\u00f3n (IR)** encontramos documentos que son relevantes para las necesidades de informaci\u00f3n del usuario. Un ejemplo popular es un motor de b\u00fasqueda web, que encuentra y presenta al usuario p\u00e1ginas relevantes para una consulta. Sin embargo, la recuperaci\u00f3n de informaci\u00f3n no se limita s\u00f3lo a la devoluci\u00f3n de documentos, sino que tambi\u00e9n puede utilizarse para otro tipo de consultas. Por ejemplo, responder preguntas cuando la consulta es una pregunta, devolver informaci\u00f3n cuando la consulta es un concepto y muchas otras aplicaciones. Un sistema de infrarrojos se compone de lo siguiente:\n", "\n", "* Un cuerpo (llamado corpus) de documentos: Una colecci\u00f3n de documentos, donde trabajar\u00e1 el RI.\n", "\n", "* Un lenguaje de consulta: Una consulta representa lo que el usuario quiere.\n", "\n", "* Resultados: Los documentos que el sistema califica como relevantes para la consulta y las necesidades del usuario.\n", "\n", "* Presentaci\u00f3n de los resultados: C\u00f3mo se presentan los resultados al usuario.\n", "\n", "\u00bfC\u00f3mo determina un sistema de IR qu\u00e9 documentos son relevantes? Podemos firmar un documento como relevante si en \u00e9l aparecen todas las palabras de la consulta, y firmarlo como irrelevante en caso contrario. Incluso podemos ampliar el lenguaje de consulta para que admita operaciones booleanas (por ejemplo, \"pintar Y pincel\") y luego firmar como relevante el resultado de la consulta para el documento. Sin embargo, esta t\u00e9cnica no proporciona un nivel de relevancia. Todos los documentos son relevantes o irrelevantes, pero en realidad algunos documentos son m\u00e1s relevantes que otros.\n", "\n", "Entonces, en lugar de un sistema de relevancia booleano, usamos una *funci\u00f3n de puntuaci\u00f3n*. Existen muchas funciones de puntuaci\u00f3n para muchas situaciones diferentes. Uno de los m\u00e1s utilizados tiene en cuenta la frecuencia de las palabras que aparecen en un documento, la frecuencia con la que aparece una palabra en todos los documentos (por ejemplo, la palabra \"a\" aparece mucho, por lo que no es muy importante) y la longitud de un documento (ya que los documentos grandes tendr\u00e1n m\u00e1s apariciones de los t\u00e9rminos de consulta, pero un documento corto con muchas apariciones parece muy relevante). Combinamos estas propiedades en una f\u00f3rmula y obtenemos una puntuaci\u00f3n num\u00e9rica para cada documento, de modo que luego podamos cuantificar la relevancia y elegir los mejores documentos.\n", "\n", "Sin embargo, estas funciones de puntuaci\u00f3n no son perfectas y se pueden mejorar. Por ejemplo, para la funci\u00f3n de puntuaci\u00f3n anterior asumimos que cada palabra es independiente. Sin embargo, ese no es el caso, ya que las palabras pueden compartir significado. Por ejemplo, las palabras \"pintor\" y \"pintores\" est\u00e1n estrechamente relacionadas. Si en una consulta tenemos la palabra \"pintor\" y en un documento aparece mucho la palabra \"pintores\", esto puede ser un indicativo de que el documento es relevante pero nos lo estamos perdiendo ya que solo buscamos \"pintor\". Hay muchas maneras de combatir esto. Uno de ellos es reducir las palabras de consulta y documento en sus ra\u00edces. Por ejemplo, tanto \"pintor\" como \"pintores\" tienen \"pintura\" como forma de ra\u00edz. Esto puede mejorar ligeramente el rendimiento de los algoritmos.\n", "\n", "Para determinar qu\u00e9 tan bueno es un sistema IR, le damos al sistema un conjunto de consultas (para las cuales conocemos las p\u00e1ginas relevantes de antemano) y registramos los resultados. Las dos medidas de rendimiento son *precisi\u00f3n* y *recuperaci\u00f3n*. La precisi\u00f3n mide la proporci\u00f3n de documentos de resultados que realmente son relevantes. La recuperaci\u00f3n mide la proporci\u00f3n de documentos relevantes (que, como se mencion\u00f3 anteriormente, conocemos de antemano) que aparecen en los documentos de resultados."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implementaci\u00f3n\n", "\n", "Puede leer el c\u00f3digo fuente ejecutando el siguiente comando:"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": true}, "outputs": [], "source": ["psource(IRSystem)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El argumento \"palabras vac\u00edas\" indica palabras en las consultas que no deben contabilizarse en los documentos. Suelen ser palabras muy comunes que no aportan ninguna informaci\u00f3n significativa sobre la relevancia de un documento.\n", "\n", "Una gu\u00eda r\u00e1pida para las funciones de la clase `IRSystem`:\n", "\n", "* `index_document`: Agrega un documento a la colecci\u00f3n de documentos (llamada `documents`), que es una lista de tuplas. Adem\u00e1s, cuente cu\u00e1ntas veces aparece cada palabra de la consulta en cada documento.\n", "\n", "* `index_collection`: indexa una colecci\u00f3n de documentos dada por `filenames`.\n", "\n", "* `query`: Devuelve una lista de `n` pares de `(score, docid)` ordenados seg\u00fan la puntuaci\u00f3n de cada documento. Tambi\u00e9n se encarga de la consulta especial \"aprender: X\", donde en lugar de la funcionalidad normal presentamos la salida del comando de terminal \"X\".\n", "\n", "* `score`: Califica un documento dado para la palabra dada usando `log(1+k)/log(1+n)`, donde `k` es el n\u00famero de palabras de consulta en un documento y `k` es el total n\u00famero de palabras del documento. Se pueden utilizar otras funciones de puntuaci\u00f3n y usted puede sobrescribir esta funci\u00f3n para adaptarla mejor a sus necesidades.\n", "\n", "* `total_score`: Calcula la suma de todas las palabras de consulta en un documento determinado.\n", "\n", "* `present`/`present_results`: Presenta los resultados como una lista.\n", "\n", "Tambi\u00e9n tenemos la clase \"Documento\" que contiene metadatos de documentos, como su t\u00edtulo, URL y n\u00famero de palabras. Se puede utilizar una clase adicional, `UnixConsultant`, para inicializar un sistema IR para manuales de comandos de Unix. Este es el ejemplo que usaremos para mostrar la implementaci\u00f3n."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Ejemplo\n", "\n", "Primero, echemos un vistazo al c\u00f3digo fuente de \"UnixConsultant\"."]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": true}, "outputs": [], "source": ["psource(UnixConsultant)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La clase crea un sistema IR con las palabras vac\u00edas \"\u00bfC\u00f3mo hago la a de?\". Podr\u00edamos agregar m\u00e1s palabras para excluir, pero las consultas que probaremos generalmente tendr\u00e1n ese formato, por lo que es conveniente. Despu\u00e9s de la inicializaci\u00f3n del sistema, obtenemos los archivos manuales y comenzamos a indexarlos.\n", "\n", "Construyamos nuestro consultor de Unix y ejecutemos una consulta:"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["0.7682667868462166 aima-data/MAN/rm.txt\n"]}], "source": ["uc = UnixConsultant()\n", "\n", "q = uc.query(\"how do I remove a file\")\n", "\n", "top_score, top_doc = q[0][0], q[0][1]\n", "print(top_score, uc.documents[top_doc].url)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preguntamos c\u00f3mo eliminar un archivo y el principal resultado fue el manual `rm` (el comando de Unix para eliminar). \u00a1Esto es exactamente lo que quer\u00edamos! Probemos con otra consulta:"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["0.7546722691607105 aima-data/MAN/diff.txt\n"]}], "source": ["q = uc.query(\"how do I delete a file\")\n", "\n", "top_score, top_doc = q[0][0], q[0][1]\n", "print(top_score, uc.documents[top_doc].url)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aunque b\u00e1sicamente pedimos lo mismo, obtuvimos un resultado superior diferente. El comando `diff` muestra las diferencias entre dos archivos. Entonces el sistema nos fall\u00f3 y nos present\u00f3 un documento irrelevante. \u00bfPorqu\u00e9 es eso? Desafortunadamente nuestro sistema IR considera cada palabra independiente. \"Eliminar\" y \"eliminar\" tienen significados similares, pero como son palabras diferentes, nuestro sistema no establecer\u00e1 la conexi\u00f3n. Entonces, el manual `diff` que menciona mucho la palabra `delete` recibe el visto bueno por delante de otros manuales, mientras que el manual `rm` no est\u00e1 en el conjunto de resultados ya que no usa la palabra en absoluto."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## EXTRACCI\u00d3N DE INFORMACI\u00d3N\n", "\n", "**Extracci\u00f3n de informaci\u00f3n (IE)** es un m\u00e9todo para encontrar apariciones de clases de objetos y relaciones en el texto. A diferencia de los sistemas IR, un sistema IE incluye nociones (limitadas) de sintaxis y sem\u00e1ntica. Si bien es dif\u00edcil extraer informaci\u00f3n de objetos en un entorno general, para dominios m\u00e1s espec\u00edficos el sistema es muy \u00fatil. Un modelo de sistema IE utiliza plantillas que coinciden con cadenas en un texto.\n", "\n", "Un ejemplo t\u00edpico de este modelo es la lectura de precios en p\u00e1ginas web. Los precios suelen aparecer despu\u00e9s de un d\u00f3lar y constan de n\u00fameros, tal vez seguidos de dos puntos decimales. Antes del precio, normalmente aparecer\u00e1 una cadena como \"precio:\". Construyamos una plantilla de muestra.\n", "\n", "Con la siguiente expresi\u00f3n regular (*regex*) podemos extraer precios del texto:\n", "\n", "`[$][0-9]+([.][0-9][0-9])?`\n", "\n", "Donde `+` significa 1 o m\u00e1s apariciones y `?` significa como m\u00e1ximo 1 aparici\u00f3n. Por lo general, una plantilla consta de un prefijo, un destino y una expresi\u00f3n regular de postfijo. En esta plantilla, la expresi\u00f3n regular del prefijo puede ser \"precio:\", la expresi\u00f3n regular de destino puede ser la expresi\u00f3n regular anterior y la expresi\u00f3n regular del postfijo puede estar vac\u00eda.\n", "\n", "Una plantilla puede coincidir con varias cadenas. Si este es el caso, necesitamos una forma de resolver las coincidencias m\u00faltiples. En lugar de tener una sola plantilla, podemos usar varias plantillas (ordenadas por prioridad) y elegir la que coincida con la plantilla de mayor prioridad. Tambi\u00e9n podemos utilizar otras formas de elegir. Para el ejemplo del d\u00f3lar, podemos elegir la coincidencia m\u00e1s cercana a la mitad num\u00e9rica de la coincidencia m\u00e1s alta. Para el texto \"Precio $90, oferta especial $70, env\u00edo $5\", elegir\u00edamos \"$70\", ya que est\u00e1 m\u00e1s cerca de la mitad de la igualaci\u00f3n m\u00e1s alta (\"$90\")."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lo anterior se llama extracci\u00f3n *basada en atributos*, donde queremos encontrar atributos en el texto (en el ejemplo, el precio). Un sistema de extracci\u00f3n m\u00e1s sofisticado tiene como objetivo tratar con m\u00faltiples objetos y las relaciones entre ellos. Cuando dicho sistema lee el texto \"$100\", deber\u00eda determinar no s\u00f3lo el precio sino tambi\u00e9n qu\u00e9 objeto tiene ese precio.\n", "\n", "Los sistemas de extracci\u00f3n de relaciones se pueden construir como una serie de aut\u00f3matas de estados finitos. Cada aut\u00f3mata recibe como entrada texto, realiza transformaciones en el texto y lo pasa al siguiente aut\u00f3mata como entrada. La configuraci\u00f3n de un aut\u00f3mata puede constar de las siguientes etapas:\n", "\n", "1. **Tokenizaci\u00f3n**: Segmenta el texto en tokens (palabras, n\u00fameros y puntuaci\u00f3n).\n", "\n", "2. **Manejo de palabras complejas**: Maneja palabras complejas como \"r\u00edndete\" o incluso nombres como \"Smile Inc.\"\n", "\n", "3. **Manejo de grupos b\u00e1sicos**: Maneja grupos de sustantivos y verbos, segmentando el texto en cadenas de verbos o sustantivos (por ejemplo, \"tuve que rendirme\").\n", "\n", "4. **Manejo de frases complejas**: Maneja frases complejas usando reglas gramaticales de estados finitos. Por ejemplo, \"Humano+Jug\u00f3 Ajedrez(\"con\" Humano+)?\" puede ser una plantilla/regla para capturar la relaci\u00f3n de alguien jugando al ajedrez con otros.\n", "\n", "5. **Fusi\u00f3n de estructuras**: Fusiona las estructuras construidas en los pasos anteriores."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Los modelos de extracci\u00f3n de informaci\u00f3n de estado finito basados \u200b\u200ben plantillas funcionan bien para dominios restringidos, pero funcionan mal a medida que el dominio se vuelve cada vez m\u00e1s general. Sin embargo, hay muchos modelos para elegir, cada uno con sus propias fortalezas y debilidades. Algunos de los modelos son los siguientes:\n", "\n", "* **Probabil\u00edstico**: Usando modelos ocultos de Markov, podemos extraer informaci\u00f3n en forma de prefijo, objetivo y sufijo de un texto determinado. Dos ventajas de usar HMM sobre plantillas es que podemos entrenar HMM a partir de datos y no necesitamos dise\u00f1ar plantillas elaboradas, y que un enfoque probabil\u00edstico se comporta bien incluso con ruido. En una expresi\u00f3n regular, si un car\u00e1cter est\u00e1 fuera de lugar, no tenemos coincidencia, mientras que con un enfoque probabil\u00edstico tenemos un proceso m\u00e1s fluido.\n", "\n", "* **Campos aleatorios condicionales**: Un problema con los HMM es la suposici\u00f3n de independencia estatal. Los CRF son muy similares a los HMM, pero no tienen la restricci\u00f3n de estos \u00faltimos. Adem\u00e1s, los CRF utilizan *funciones de caracter\u00edsticas*, que act\u00faan como pesos de transici\u00f3n. Por ejemplo, si para la observaci\u00f3n $e_{i}$ y el estado $x_{i}$ tenemos que $e_{i}$ es \"correr\" y $x_{i}$ es el estado ATLETA, podemos tener $f( x_{i}, e_{i}) = 1$ e igual a 0 en caso contrario. Podemos usar m\u00faltiples funciones superpuestas e incluso podemos usar funciones para transiciones de estado. Las funciones de caracter\u00edsticas no tienen que ser binarias (como en el ejemplo anterior), pero tambi\u00e9n pueden tener un valor real. Adem\u00e1s, podemos usar cualquier $e$ para la funci\u00f3n, no solo la observaci\u00f3n actual. Para reunirlo todo, sopesamos una transici\u00f3n seg\u00fan la suma de caracter\u00edsticas.\n", "\n", "* **Extracci\u00f3n de Ontolog\u00eda**: Este es un m\u00e9todo para recopilar informaci\u00f3n y hechos en un dominio general. Un hecho puede tener la forma \"NP es NP\", donde \"NP\" denota una frase nominal. Por ejemplo, \"El conejo es un mam\u00edfero\"."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## DECODIFICADORES\n", "\n", "### Introducci\u00f3n\n", "\n", "En esta secci\u00f3n intentaremos decodificar texto cifrado utilizando modelos de texto probabil\u00edsticos. Un texto cifrado se obtiene cifrando un mensaje de texto. Este cifrado nos permite comunicarnos de forma segura, ya que cualquiera que tenga acceso al texto cifrado pero no sepa decodificarlo no podr\u00e1 leer el mensaje. Restringiremos nuestro estudio a <b>Cifrados de sustituci\u00f3n monoalfab\u00e9ticos</b>. Estas son formas primitivas de cifrado en las que cada letra del texto del mensaje (tambi\u00e9n conocido como texto sin formato) se reemplaza por otra letra del alfabeto.\n", "\n", "### Descodificador de cambios\n", "\n", "#### El cifrado C\u00e9sar\n", "\n", "El cifrado C\u00e9sar, tambi\u00e9n conocido como cifrado por desplazamiento, es una forma de cifrado por sustituci\u00f3n monoalfab\u00e9tico en el que cada letra se <i>desplaza</i> en un valor fijo. Un cambio de <b>`n`</b> en este contexto significa que cada letra del texto sin formato se reemplaza con una letra correspondiente a `n` letras del alfabeto. Por ejemplo, el texto sin formato `\"ABCDWXYZ\"` desplazado por `3` produce `\"DEFGZABC\"`. Observe c\u00f3mo \"X\" se convirti\u00f3 en \"A\". Esto se debe a que el alfabeto es c\u00edclico, es decir, la letra despu\u00e9s de la \u00faltima letra del alfabeto, \"Z\", es la primera letra del alfabeto: \"A\"."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["DEFGZABC\n"]}], "source": ["plaintext = \"ABCDWXYZ\"\n", "ciphertext = shift_encode(plaintext, 3)\n", "print(ciphertext)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Decodificando un cifrado C\u00e9sar\n", "\n", "Para decodificar un cifrado C\u00e9sar aprovechamos el hecho de que no todas las letras del alfabeto se utilizan por igual. Algunas letras se utilizan m\u00e1s que otras y es m\u00e1s probable que algunos pares de letras aparezcan juntas. A un par de letras consecutivas lo llamamos <b>bigrama</b>."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['th', 'hi', 'is', 's ', ' i', 'is', 's ', ' a', 'a ', ' s', 'se', 'en', 'nt', 'te', 'en', 'nc', 'ce']\n"]}], "source": ["print(bigrams('this is a sentence'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Usamos `CountingProbDist` para obtener la distribuci\u00f3n de probabilidad de bigramas. En el alfabeto latino consta de s\u00f3lo \"26\" letras. Esto limita el n\u00famero total de sustituciones posibles a \"26\". Invertimos la codificaci\u00f3n de desplazamiento para una `n` dada y verificamos qu\u00e9 tan probable es usando la distribuci\u00f3n de bigram. Probamos todos los valores `26` de `n`, es decir, desde `n = 0` hasta `n = 26` y usamos el valor de `n` que proporciona el texto sin formato m\u00e1s probable."]}, {"cell_type": "code", "execution_count": 7, "metadata": {"collapsed": true}, "outputs": [], "source": ["%psource ShiftDecoder"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Ejemplo\n", "\n", "Codifiquemos un mensaje secreto usando el cifrado C\u00e9sar y luego intentemos decodificarlo usando `ShiftDecoder`. Usaremos nuevamente `flatland.txt` para construir el modelo de texto."]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The code is \"Guvf vf n frperg zrffntr\"\n"]}], "source": ["plaintext = \"This is a secret message\"\n", "ciphertext = shift_encode(plaintext, 13)\n", "print('The code is', '\"' + ciphertext + '\"')"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The decoded message is \"This is a secret message\"\n"]}], "source": ["flatland = open_data(\"EN-text/flatland.txt\").read()\n", "decoder = ShiftDecoder(flatland)\n", "\n", "decoded_message = decoder.decode(ciphertext)\n", "print('The decoded message is', '\"' + decoded_message + '\"')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Decodificador de permutaci\u00f3n\n", "Intentemos ahora decodificar mensajes cifrados mediante un cifrado de sustituci\u00f3n monoalfab\u00e9tico general. Las letras del alfabeto se pueden reemplazar por cualquier permutaci\u00f3n de letras. Por ejemplo, si el alfabeto constaba de `{A B C}`, entonces se puede reemplazar por `{A C B}`, `{B A C}`, `{B C A}`, `{C A B}`, `{C B A}` o incluso `{A B C}` en s\u00ed. Supongamos que elegimos la permutaci\u00f3n `{C B A}`, entonces el texto plano `\"CAB BA AAC\"` se convertir\u00eda en `\"ACB BC CCA\"`. Podemos ver que el cifrado C\u00e9sar tambi\u00e9n es una forma de cifrado por permutaci\u00f3n donde la permutaci\u00f3n es una permutaci\u00f3n c\u00edclica. A diferencia del cifrado C\u00e9sar, no es posible intentar todas las permutaciones posibles. El n\u00famero de permutaciones posibles en el alfabeto latino es `26!`, que es del orden $10^{26}$. Utilizamos algoritmos de b\u00fasqueda de gr\u00e1ficos para buscar una permutaci\u00f3n \"buena\"."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(PermutationDecoder)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cada estado/nodo en el gr\u00e1fico se representa como un mapa letra a letra. Si no hay ninguna asignaci\u00f3n para una letra, significa que la letra no ha cambiado en la permutaci\u00f3n. Estos mapas se almacenan como diccionarios. Cada diccionario es una permutaci\u00f3n \"potencial\". Usamos la palabra \"potencial\" porque cada diccionario no necesariamente representa una permutaci\u00f3n v\u00e1lida ya que una permutaci\u00f3n no puede tener elementos repetidos. Por ejemplo, el diccionario `{'A': 'B', 'C': 'X'}` no es v\u00e1lido porque `'A'` se reemplaza por `'B'`, pero tambi\u00e9n lo es `'B'` porque el El diccionario no tiene una asignaci\u00f3n para \"B\". Dos diccionarios tambi\u00e9n pueden representar la misma permutaci\u00f3n, p. `{'A': 'C', 'C': 'A'}` y `{'A': 'C', 'B': 'B', 'C': 'A'}` representan lo mismo permutaci\u00f3n donde `'A'` y `'C'` se intercambian y todas las dem\u00e1s letras permanecen inalteradas. Para garantizar que obtengamos una permutaci\u00f3n v\u00e1lida, un estado objetivo debe asignar todas las letras del alfabeto. Tambi\u00e9n evitamos repeticiones en la permutaci\u00f3n al permitir solo aquellas acciones que van a un nuevo estado/nodo en el que la letra reci\u00e9n agregada al diccionario se asigna a una letra no asignada anteriormente. Estas dos reglas juntas aseguran que el diccionario de un estado objetivo representar\u00e1 una permutaci\u00f3n v\u00e1lida.\n", "La puntuaci\u00f3n de un estado se determina utilizando puntuaciones de palabras, puntuaciones de unigramas y puntuaciones de bigramas. Experimente con diferentes ponderaciones para puntuaciones de palabras, unigramas y bigramas y vea c\u00f3mo afectan la decodificaci\u00f3n."]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\"ahed world\" decodes to \"shed could\"\n", "\"ahed woxld\" decodes to \"shew atiow\"\n"]}], "source": ["ciphertexts = ['ahed world', 'ahed woxld']\n", "\n", "pd = PermutationDecoder(canonicalize(flatland))\n", "for ctext in ciphertexts:\n", "    print('\"{}\" decodes to \"{}\"'.format(ctext, pd.decode(ctext)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Como se desprende del ejemplo anterior, la decodificaci\u00f3n de permutaciones utilizando la mejor primera b\u00fasqueda es sensible al texto inicial. Esto se debe a que no s\u00f3lo el diccionario final, con sustituciones de todas las letras, debe tener una buena puntuaci\u00f3n, sino tambi\u00e9n los diccionarios intermedios. Podr\u00eda pensar en ello como realizar una b\u00fasqueda local encontrando sustituciones para cada letra una por una. Podr\u00edamos obtener resultados muy diferentes cambiando incluso una sola letra porque esa letra podr\u00eda ser un factor decisivo para seleccionar la sustituci\u00f3n en las primeras etapas, lo que se multiplica y afecta las etapas posteriores. Para mejorar la b\u00fasqueda, podemos utilizar diferentes definiciones de puntuaci\u00f3n en diferentes etapas y optimizar qu\u00e9 letra sustituir primero."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.3"}}, "nbformat": 4, "nbformat_minor": 1}