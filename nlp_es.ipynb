{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# PROCESAMIENTO NATURAL DEL LENGUAJE\n", "\n", "Este cuaderno cubre los cap\u00edtulos 22 y 23 del libro *Inteligencia artificial: un enfoque moderno*, tercera edici\u00f3n. Las implementaciones de los algoritmos se pueden encontrar en [nlp.py](https://github.com/aimacode/aima-python/blob/master/nlp.py).\n", "\n", "Ejecute la siguiente celda para importar el c\u00f3digo del m\u00f3dulo y comenzar."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import nlp\n", "from nlp import Page, HITS\n", "from nlp import Lexicon, Rules, Grammar, ProbLexicon, ProbRules, ProbGrammar\n", "from nlp import CYK_parse, Chart\n", "\n", "from notebook import psource"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## CONTENIDO\n", "\n", "* Descripci\u00f3n general\n", "* Idiomas\n", "* GOLPES\n", "* Respuesta a preguntas\n", "* An\u00e1lisis CYK\n", "* An\u00e1lisis de gr\u00e1ficos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## DESCRIPCI\u00d3N GENERAL\n", "\n", "**El procesamiento del lenguaje natural (PNL)** es un campo de la IA que se ocupa de comprender, analizar y utilizar lenguajes naturales. Este campo se considera un campo de estudio dif\u00edcil pero intrigante, ya que est\u00e1 relacionado con el funcionamiento de los humanos y sus lenguajes.\n", "\n", "Las aplicaciones del campo incluyen traducci\u00f3n, reconocimiento de voz, segmentaci\u00f3n de temas, extracci\u00f3n y recuperaci\u00f3n de informaci\u00f3n y mucho m\u00e1s.\n", "\n", "A continuaci\u00f3n echamos un vistazo a algunos algoritmos en el campo. Sin embargo, antes de entrar en materia, echaremos un vistazo a una forma de lenguaje muy \u00fatil: los lenguajes **libres de contexto**. Aunque son un poco restrictivos, se han utilizado mucho en investigaciones sobre el procesamiento del lenguaje natural."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## IDIOMAS\n", "\n", "Los idiomas pueden representarse mediante un conjunto de reglas gramaticales sobre un l\u00e9xico de palabras. Diferentes idiomas pueden representarse mediante diferentes tipos de gram\u00e1tica, pero en Procesamiento del Lenguaje Natural nos interesan principalmente las gram\u00e1ticas libres de contexto.\n", "\n", "### Gram\u00e1ticas libres de contexto\n", "\n", "Muchos lenguajes naturales y de programaci\u00f3n se pueden representar mediante una **Gram\u00e1tica libre de contexto (CFG)**. Un CFG es una gram\u00e1tica que tiene un \u00fanico s\u00edmbolo no terminal en el lado izquierdo. Eso significa que un no terminal puede reemplazarse por el lado derecho de la regla independientemente del contexto. Un ejemplo de CFG:\n", "\n", "```\n", "S -> aSb | \u03b5\n", "```\n", "\n", "Eso significa que `S` se puede reemplazar por `aSb` o `\u03b5` (con `\u03b5` denotamos la cadena vac\u00eda). El l\u00e9xico del idioma est\u00e1 compuesto por las terminales `a` y `b`, mientras que con `S` denotamos el s\u00edmbolo no terminal. En general, los no terminales est\u00e1n en may\u00fascula, mientras que los terminales no, y normalmente denominamos al no terminal inicial \"S\". El lenguaje generado por la gram\u00e1tica anterior es el lenguaje a<sup>n</sup>b<sup>n</sup> para n mayor o igual que 1."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Gram\u00e1tica probabil\u00edstica libre de contexto\n", "\n", "Si bien un CFG simple puede resultar muy \u00fatil, es posible que deseemos conocer la probabilidad de que se produzca cada regla. Arriba, no sabemos si es m\u00e1s probable que \"S\" sea reemplazado por \"aSb\" o \"\u03b5\". **Las gram\u00e1ticas probabil\u00edsticas libres de contexto (PCFG)** est\u00e1n dise\u00f1adas para satisfacer exactamente esa necesidad. Cada regla tiene una probabilidad, dada entre par\u00e9ntesis, y las probabilidades de una regla suman 1:\n", "\n", "```\n", "S -> aSb [0.7] | \u03b5 [0.3]\n", "```\n", "\n", "Ahora sabemos que es m\u00e1s probable que \"S\" sea reemplazado por \"aSb\" que por \"\u03b5\".\n", "\n", "Un problema con los *PCFG* es c\u00f3mo asignaremos las distintas probabilidades a las reglas. Podr\u00edamos utilizar nuestro conocimiento como humanos para asignar las probabilidades, pero esa es una tarea laboriosa y propensa a errores. En cambio, podemos *aprender* las probabilidades a partir de los datos. Los datos se clasifican como etiquetados (con oraciones correctamente analizadas, generalmente denominadas **banco de \u00e1rboles**) o sin etiquetar (con solo nombres de categor\u00edas l\u00e9xicas y sint\u00e1cticas).\n", "\n", "Con datos etiquetados, podemos simplemente contar las ocurrencias. Para la gram\u00e1tica anterior, si tenemos 100 reglas `S` y 30 de ellas son de la forma `S -> \u03b5`, asignamos una probabilidad de 0,3 a la transformaci\u00f3n.\n", "\n", "Con datos sin etiquetar tenemos que aprender tanto las reglas gramaticales como la probabilidad de cada regla. Podemos utilizar muchos enfoques, uno de ellos, el algoritmo **adentro-afuera**. Utiliza un enfoque de programaci\u00f3n din\u00e1mica, que primero encuentra la probabilidad de que cada regla genere una subcadena y luego estima la probabilidad de cada regla."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Forma normal de Chomsky\n", "\n", "Una gram\u00e1tica est\u00e1 en forma normal de Chomsky (o **CNF**, que no debe confundirse con *forma normal conjuntiva*) si sus reglas son una de las tres:\n", "\n", "* `X -> Y Z`\n", "*`A->a`\n", "*`S->e`\n", "\n", "Donde *X*, *Y*, *Z*, *A* son no terminales, *a* es un terminal, *\u03b5* es la cadena vac\u00eda y *S* es el s\u00edmbolo inicial (el s\u00edmbolo inicial no debe ser que aparece en el lado derecho de las reglas). Tenga en cuenta que puede haber varias reglas para cada no terminal del lado izquierdo, siempre que sigan lo anterior. Por ejemplo, una regla para *X* podr\u00eda ser: `X -> Y Z | A B | un | b`.\n", "\n", "Por supuesto, tambi\u00e9n podemos tener un *CNF* con probabilidades.\n", "\n", "Este tipo de gram\u00e1tica puede parecer restrictiva, pero se puede demostrar que cualquier gram\u00e1tica libre de contexto se puede convertir a CNF."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### L\u00e9xico\n", "\n", "El l\u00e9xico de un idioma se define como una lista de palabras permitidas. Estas palabras se agrupan en las clases habituales: `verbos`, `sustantivos`, `adjetivos`, `adverbios`, `pronombres`, `nombres`, `art\u00edculos`, `preposiciones` y `conjunciones`. Para las primeras cinco clases es imposible enumerar todas las palabras, ya que continuamente se agregan palabras en las clases. Recientemente se agreg\u00f3 \"google\" a la lista de verbos, y palabras como esa seguir\u00e1n apareciendo y agreg\u00e1ndose a las listas. Por esa raz\u00f3n, estas primeras cinco categor\u00edas se denominan **clases abiertas**. El resto de categor\u00edas tienen muchas menos palabras y mucho menos desarrollo. Si bien palabras como \"t\u00fa\" se usaban com\u00fanmente en el pasado, pero su uso ha disminuido casi por completo, la mayor\u00eda de los cambios tardan muchas d\u00e9cadas o siglos en manifestarse, por lo que podemos asumir con seguridad que las categor\u00edas permanecer\u00e1n est\u00e1ticas en el futuro previsible. Por lo tanto, estas categor\u00edas se denominan **clases cerradas**.\n", "\n", "Un l\u00e9xico de ejemplo para un PCFG (tenga en cuenta que tambi\u00e9n se pueden usar otras clases seg\u00fan el idioma, como `d\u00edgitos` o `RelPro` para pronombre relativo):\n", "\n", "```\n", "Verb -> is [0.3] | say [0.1] | are [0.1] | ...\n", "Noun -> robot [0.1] | sheep [0.05] | fence [0.05] | ...\n", "Adjective -> good [0.1] | new [0.1] | sad [0.05] | ...\n", "Adverb -> here [0.1] | lightly [0.05] | now [0.05] | ...\n", "Pronoun -> me [0.1] | you [0.1] | he [0.05] | ...\n", "RelPro -> that [0.4] | who [0.2] | which [0.2] | ...\n", "Name -> john [0.05] | mary [0.05] | peter [0.01] | ...\n", "Article -> the [0.35] | a [0.25] | an [0.025] | ...\n", "Preposition -> to [0.25] | in [0.2] | at [0.1] | ...\n", "Conjunction -> and [0.5] | or [0.2] | but [0.2] | ...\n", "Digit -> 1 [0.3] | 2 [0.2] | 0 [0.2] | ...\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Gram\u00e1tica\n", "\n", "Con las gram\u00e1ticas combinamos palabras del l\u00e9xico en frases v\u00e1lidas. Una gram\u00e1tica se compone de **reglas gramaticales**. Cada regla transforma el lado izquierdo de la regla en el lado derecho. Por ejemplo, `A -> B` significa que `A` se transforma en `B`. Construyamos una gram\u00e1tica para el idioma que comenzamos a construir con el l\u00e9xico. Usaremos un PCFG.\n", "\n", "```\n", "S -> NP VP [0.9] | S Conjunction S [0.1]\n", "\n", "NP -> Pronoun [0.3] | Name [0.1] | Noun [0.1] | Article Noun [0.25] |\n", "      Article Adjs Noun [0.05] | Digit [0.05] | NP PP [0.1] |\n", "      NP RelClause [0.05]\n", "\n", "VP -> Verb [0.4] | VP NP [0.35] | VP Adjective [0.05] | VP PP [0.1]\n", "      VP Adverb [0.1]\n", "\n", "Adjs -> Adjective [0.8] | Adjective Adjs [0.2]\n", "\n", "PP -> Preposition NP [1.0]\n", "\n", "RelClause -> RelPro VP [1.0]\n", "```\n", "\n", "Algunas frases v\u00e1lidas que produce la gram\u00e1tica: \"`mary est\u00e1 triste`\", \"`eres un robot`\" y \"`a ella le gusta mary y una buena valla`\".\n", "\n", "\u00bfQu\u00e9 pasar\u00eda si quisi\u00e9ramos comprobar si la frase \"`mary is sad`\" es realmente una oraci\u00f3n v\u00e1lida? Podemos usar un **\u00e1rbol de an\u00e1lisis** para demostrar constructivamente que una cadena de palabras es una frase v\u00e1lida en el idioma dado e incluso calcular la probabilidad de generaci\u00f3n de la oraci\u00f3n.\n", "\n", "![parse_tree](images/parse_tree.png)\n", "\n", "La probabilidad de todo el \u00e1rbol se puede calcular multiplicando las probabilidades de cada transformaci\u00f3n de regla individual: `0,9 * 0,1 * 0,05 * 0,05 * 0,4 * 0,05 * 0,3 = 0,00000135`.\n", "\n", "Para conservar espacio, tambi\u00e9n podemos escribir el \u00e1rbol en forma lineal:\n", "\n", "[S [NP [Nombre **mar\u00eda**]] [VP [VP [Verbo **es**]] [Adjetivo **triste**]]]\n", "\n", "Desafortunadamente, la gram\u00e1tica actual **sobregenera**, es decir, crea oraciones que no son gramaticalmente correctas (seg\u00fan el idioma ingl\u00e9s), como \"`the valla are john that say`\". Tambi\u00e9n **subgenera**, lo que significa que hay oraciones v\u00e1lidas que no genera, como \"`\u00e9l cree que Mar\u00eda est\u00e1 triste`\"."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implementaci\u00f3n\n", "\n", "En el m\u00f3dulo tenemos implementaci\u00f3n para gram\u00e1ticas probabil\u00edsticas y no probabil\u00edsticas. Ambas implementaciones siguen el mismo formato. Hay funciones para el l\u00e9xico y las reglas que se pueden combinar para crear un objeto gramatical.\n", "\n", "#### No probabil\u00edstico\n", "\n", "Ejecute la siguiente celda para ver las implementaciones:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(Lexicon, Rules, Grammar)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Construyamos un l\u00e9xico y una gram\u00e1tica para el lenguaje anterior:"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Lexicon {'Adverb': ['here', 'lightly', 'now'], 'Verb': ['is', 'say', 'are'], 'Digit': ['1', '2', '0'], 'RelPro': ['that', 'who', 'which'], 'Conjunction': ['and', 'or', 'but'], 'Name': ['john', 'mary', 'peter'], 'Pronoun': ['me', 'you', 'he'], 'Article': ['the', 'a', 'an'], 'Noun': ['robot', 'sheep', 'fence'], 'Adjective': ['good', 'new', 'sad'], 'Preposition': ['to', 'in', 'at']}\n", "\n", "Rules: {'RelClause': [['RelPro', 'VP']], 'Adjs': [['Adjective'], ['Adjective', 'Adjs']], 'NP': [['Pronoun'], ['Name'], ['Noun'], ['Article', 'Noun'], ['Article', 'Adjs', 'Noun'], ['Digit'], ['NP', 'PP'], ['NP', 'RelClause']], 'S': [['NP', 'VP'], ['S', 'Conjunction', 'S']], 'VP': [['Verb'], ['VP', 'NP'], ['VP', 'Adjective'], ['VP', 'PP'], ['VP', 'Adverb']], 'PP': [['Preposition', 'NP']]}\n"]}], "source": ["lexicon = Lexicon(\n", "    Verb = \"is | say | are\",\n", "    Noun = \"robot | sheep | fence\",\n", "    Adjective = \"good | new | sad\",\n", "    Adverb = \"here | lightly | now\",\n", "    Pronoun = \"me | you | he\",\n", "    RelPro = \"that | who | which\",\n", "    Name = \"john | mary | peter\",\n", "    Article = \"the | a | an\",\n", "    Preposition = \"to | in | at\",\n", "    Conjunction = \"and | or | but\",\n", "    Digit = \"1 | 2 | 0\"\n", ")\n", "\n", "print(\"Lexicon\", lexicon)\n", "\n", "rules = Rules(\n", "    S = \"NP VP | S Conjunction S\",\n", "    NP = \"Pronoun | Name | Noun | Article Noun \\\n", "          | Article Adjs Noun | Digit | NP PP | NP RelClause\",\n", "    VP = \"Verb | VP NP | VP Adjective | VP PP | VP Adverb\",\n", "    Adjs = \"Adjective | Adjective Adjs\",\n", "    PP = \"Preposition NP\",\n", "    RelClause = \"RelPro VP\"\n", ")\n", "\n", "print(\"\\nRules:\", rules)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ambas funciones devuelven un diccionario con claves en el lado izquierdo de las reglas. Para el l\u00e9xico, los valores son los terminales de cada no terminal del lado izquierdo, mientras que para las reglas los valores son los lados derechos como listas.\n", "\n", "Ahora podemos usar las variables `l\u00e9xico` y `reglas` para construir una gram\u00e1tica. Despu\u00e9s de haberlo hecho, podemos encontrar las transformaciones de un no terminal (el `Sustantivo`, el `Verbo` y las otras clases b\u00e1sicas **no** cuentan como no terminales adecuados en la implementaci\u00f3n). Tambi\u00e9n podemos comprobar si una palabra est\u00e1 en una clase particular."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["How can we rewrite 'VP'? [['Verb'], ['VP', 'NP'], ['VP', 'Adjective'], ['VP', 'PP'], ['VP', 'Adverb']]\n", "Is 'the' an article? True\n", "Is 'here' a noun? False\n"]}], "source": ["grammar = Grammar(\"A Simple Grammar\", rules, lexicon)\n", "\n", "print(\"How can we rewrite 'VP'?\", grammar.rewrites_for('VP'))\n", "print(\"Is 'the' an article?\", grammar.isa('the', 'Article'))\n", "print(\"Is 'here' a noun?\", grammar.isa('here', 'Noun'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Si la gram\u00e1tica est\u00e1 en la forma normal de Chomsky, podemos llamar a la funci\u00f3n de clase `cnf_rules` para obtener todas las reglas en la forma `(X, Y, Z)` para cada regla `X -> Y Z`. Sin embargo, dado que la gram\u00e1tica anterior no est\u00e1 en *CNF*, tenemos que crear una nueva."]}, {"cell_type": "code", "execution_count": 4, "metadata": {"collapsed": true}, "outputs": [], "source": ["E_Chomsky = Grammar(\"E_Prob_Chomsky\", # A Grammar in Chomsky Normal Form\n", "        Rules(\n", "           S = \"NP VP\",\n", "           NP = \"Article Noun | Adjective Noun\",\n", "           VP = \"Verb NP | Verb Adjective\",\n", "        ),\n", "        Lexicon(\n", "           Article = \"the | a | an\",\n", "           Noun = \"robot | sheep | fence\",\n", "           Adjective = \"good | new | sad\",\n", "           Verb = \"is | say | are\"\n", "        ))"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[('S', 'NP', 'VP'), ('VP', 'Verb', 'NP'), ('VP', 'Verb', 'Adjective'), ('NP', 'Article', 'Noun'), ('NP', 'Adjective', 'Noun')]\n"]}], "source": ["print(E_Chomsky.cnf_rules())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finalmente, podemos generar frases aleatorias usando nuestra gram\u00e1tica. La mayor\u00eda de ellos ser\u00e1n un completo galimat\u00edas y caer\u00e1n bajo las frases sobregeneradas de la gram\u00e1tica. Esto demuestra que en la gram\u00e1tica las frases v\u00e1lidas son muchas menos que las sobregeneradas."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": ["'sheep that say here mary are the sheep at 2'"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["grammar.generate_random('S')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Probabil\u00edstico\n", "\n", "Las gram\u00e1ticas probabil\u00edsticas siguen el mismo enfoque. Toman como entrada una cadena, se ensamblan a partir de una gram\u00e1tica y un l\u00e9xico y pueden generar oraciones aleatorias (dando la probabilidad de la oraci\u00f3n). La principal diferencia es que en el l\u00e9xico tenemos tuplas (terminal, probabilidad) en lugar de cadenas y para las reglas tenemos una lista de tuplas (lista de no terminales, probabilidad) en lugar de una lista de listas de no terminales.\n", "\n", "Ejecute las celdas para leer el c\u00f3digo:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(ProbLexicon, ProbRules, ProbGrammar)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Construyamos un l\u00e9xico y reglas para la gram\u00e1tica probabil\u00edstica:"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Lexicon {'Noun': [('robot', 0.4), ('sheep', 0.4), ('fence', 0.2)], 'Name': [('john', 0.4), ('mary', 0.4), ('peter', 0.2)], 'Adverb': [('here', 0.6), ('lightly', 0.1), ('now', 0.3)], 'Digit': [('0', 0.35), ('1', 0.35), ('2', 0.3)], 'Adjective': [('good', 0.5), ('new', 0.2), ('sad', 0.3)], 'Pronoun': [('me', 0.3), ('you', 0.4), ('he', 0.3)], 'Article': [('the', 0.5), ('a', 0.25), ('an', 0.25)], 'Preposition': [('to', 0.4), ('in', 0.3), ('at', 0.3)], 'Verb': [('is', 0.5), ('say', 0.3), ('are', 0.2)], 'Conjunction': [('and', 0.5), ('or', 0.2), ('but', 0.3)], 'RelPro': [('that', 0.5), ('who', 0.3), ('which', 0.2)]}\n", "\n", "Rules: {'S': [(['NP', 'VP'], 0.6), (['S', 'Conjunction', 'S'], 0.4)], 'RelClause': [(['RelPro', 'VP'], 1.0)], 'VP': [(['Verb'], 0.3), (['VP', 'NP'], 0.2), (['VP', 'Adjective'], 0.25), (['VP', 'PP'], 0.15), (['VP', 'Adverb'], 0.1)], 'Adjs': [(['Adjective'], 0.5), (['Adjective', 'Adjs'], 0.5)], 'PP': [(['Preposition', 'NP'], 1.0)], 'NP': [(['Pronoun'], 0.2), (['Name'], 0.05), (['Noun'], 0.2), (['Article', 'Noun'], 0.15), (['Article', 'Adjs', 'Noun'], 0.1), (['Digit'], 0.05), (['NP', 'PP'], 0.15), (['NP', 'RelClause'], 0.1)]}\n"]}], "source": ["lexicon = ProbLexicon(\n", "    Verb = \"is [0.5] | say [0.3] | are [0.2]\",\n", "    Noun = \"robot [0.4] | sheep [0.4] | fence [0.2]\",\n", "    Adjective = \"good [0.5] | new [0.2] | sad [0.3]\",\n", "    Adverb = \"here [0.6] | lightly [0.1] | now [0.3]\",\n", "    Pronoun = \"me [0.3] | you [0.4] | he [0.3]\",\n", "    RelPro = \"that [0.5] | who [0.3] | which [0.2]\",\n", "    Name = \"john [0.4] | mary [0.4] | peter [0.2]\",\n", "    Article = \"the [0.5] | a [0.25] | an [0.25]\",\n", "    Preposition = \"to [0.4] | in [0.3] | at [0.3]\",\n", "    Conjunction = \"and [0.5] | or [0.2] | but [0.3]\",\n", "    Digit = \"0 [0.35] | 1 [0.35] | 2 [0.3]\"\n", ")\n", "\n", "print(\"Lexicon\", lexicon)\n", "\n", "rules = ProbRules(\n", "    S = \"NP VP [0.6] | S Conjunction S [0.4]\",\n", "    NP = \"Pronoun [0.2] | Name [0.05] | Noun [0.2] | Article Noun [0.15] \\\n", "        | Article Adjs Noun [0.1] | Digit [0.05] | NP PP [0.15] | NP RelClause [0.1]\",\n", "    VP = \"Verb [0.3] | VP NP [0.2] | VP Adjective [0.25] | VP PP [0.15] | VP Adverb [0.1]\",\n", "    Adjs = \"Adjective [0.5] | Adjective Adjs [0.5]\",\n", "    PP = \"Preposition NP [1]\",\n", "    RelClause = \"RelPro VP [1]\"\n", ")\n", "\n", "print(\"\\nRules:\", rules)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Usemos lo anterior para ensamblar nuestra gram\u00e1tica probabil\u00edstica y ejecutar algunas consultas simples:"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["How can we rewrite 'VP'? [(['Verb'], 0.3), (['VP', 'NP'], 0.2), (['VP', 'Adjective'], 0.25), (['VP', 'PP'], 0.15), (['VP', 'Adverb'], 0.1)]\n", "Is 'the' an article? True\n", "Is 'here' a noun? False\n"]}], "source": ["grammar = ProbGrammar(\"A Simple Probabilistic Grammar\", rules, lexicon)\n", "\n", "print(\"How can we rewrite 'VP'?\", grammar.rewrites_for('VP'))\n", "print(\"Is 'the' an article?\", grammar.isa('the', 'Article'))\n", "print(\"Is 'here' a noun?\", grammar.isa('here', 'Noun'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Si tenemos una gram\u00e1tica en *CNF*, podemos obtener una lista de todas las reglas. Creemos una gram\u00e1tica en el formulario e imprimamos las reglas *CNF*:"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["E_Prob_Chomsky = ProbGrammar(\"E_Prob_Chomsky\", # A Probabilistic Grammar in CNF\n", "                             ProbRules(\n", "                                S = \"NP VP [1]\",\n", "                                NP = \"Article Noun [0.6] | Adjective Noun [0.4]\",\n", "                                VP = \"Verb NP [0.5] | Verb Adjective [0.5]\",\n", "                             ),\n", "                             ProbLexicon(\n", "                                Article = \"the [0.5] | a [0.25] | an [0.25]\",\n", "                                Noun = \"robot [0.4] | sheep [0.4] | fence [0.2]\",\n", "                                Adjective = \"good [0.5] | new [0.2] | sad [0.3]\",\n", "                                Verb = \"is [0.5] | say [0.3] | are [0.2]\"\n", "                             ))"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[('S', 'NP', 'VP', 1.0), ('VP', 'Verb', 'NP', 0.5), ('VP', 'Verb', 'Adjective', 0.5), ('NP', 'Article', 'Noun', 0.6), ('NP', 'Adjective', 'Noun', 0.4)]\n"]}], "source": ["print(E_Prob_Chomsky.cnf_rules())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Por \u00faltimo, podemos generar oraciones aleatorias a partir de esta gram\u00e1tica. La funci\u00f3n `prob_generaci\u00f3n` devuelve una tupla (oraci\u00f3n, probabilidad)."]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["an good sad sheep to 1 is\n", "3.54375e-08\n"]}], "source": ["sentence, prob = grammar.generate_random('S')\n", "print(sentence)\n", "print(prob)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Al igual que con las gram\u00e1ticas no probabil\u00edsticas, \u00e9sta en su mayor\u00eda genera en exceso. Tambi\u00e9n puedes ver que la probabilidad es muy, muy baja, lo que significa que hay un mont\u00f3n de oraciones generables (en este caso infinitas, ya que tenemos recursividad; observa c\u00f3mo \"VP\" puede producir otro \"VP\", por ejemplo)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## GOLPES\n", "\n", "### Descripci\u00f3n general\n", "\n", "**La b\u00fasqueda de temas inducida por hiperv\u00ednculos** (o HITS para abreviar) es un algoritmo para la recuperaci\u00f3n de informaci\u00f3n y la clasificaci\u00f3n de p\u00e1ginas. Puede leer m\u00e1s sobre la recuperaci\u00f3n de informaci\u00f3n en [text notebook](https://github.com/aimacode/aima-python/blob/master/text.ipynb). Esencialmente, dada una colecci\u00f3n de documentos y la consulta de un usuario, dichos sistemas devuelven al usuario los documentos m\u00e1s relevantes para sus necesidades. El algoritmo HITS difiere de muchos otros algoritmos de clasificaci\u00f3n similares (como el *Pagerank* de Google) ya que las clasificaciones de las p\u00e1ginas en este algoritmo dependen de la consulta dada. Esto significa que para cada nueva consulta las p\u00e1ginas de resultados deben calcularse nuevamente. Este coste puede resultar prohibitivo para muchos motores de b\u00fasqueda modernos, por lo que muchos evitan este enfoque.\n", "\n", "HITS primero encuentra una lista de p\u00e1ginas relevantes para la consulta y luego agrega p\u00e1ginas que enlazan o est\u00e1n enlazadas desde estas p\u00e1ginas. Una vez construido el conjunto, definimos dos valores para cada p\u00e1gina. **Autoridad** en la consulta, el grado de p\u00e1ginas del conjunto relevante que enlazan con ella y **centro** de la consulta, el grado en que apunta a p\u00e1ginas autorizadas en el conjunto. Como no queremos simplemente contar el n\u00famero de enlaces de una p\u00e1gina a otras p\u00e1ginas, sino que tambi\u00e9n queremos tener en cuenta la calidad de las p\u00e1ginas enlazadas, actualizamos los valores de centro y autoridad de una p\u00e1gina de la siguiente manera, hasta convergencia:\n", "\n", "* Puntuaci\u00f3n del centro = La suma de las puntuaciones de autoridad de las p\u00e1ginas a las que enlaza.\n", "\n", "* Puntuaci\u00f3n de autoridad = La suma de las puntuaciones centrales de las p\u00e1ginas desde las que est\u00e1 vinculado.\n", "\n", "Por lo tanto, cuanto mayor sea la calidad de las p\u00e1ginas a las que est\u00e1 vinculada una p\u00e1gina, mayores ser\u00e1n sus puntuaciones.\n", "\n", "Luego normalizamos las puntuaciones dividiendo cada puntuaci\u00f3n por la suma de los cuadrados de las puntuaciones respectivas de todas las p\u00e1ginas. Cuando los valores convergen, devolvemos las p\u00e1ginas mejor valoradas. Tenga en cuenta que debido a que normalizamos los valores, se garantiza que el algoritmo converger\u00e1."]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["### Implementaci\u00f3n\n", "\n", "El c\u00f3digo fuente del algoritmo se proporciona a continuaci\u00f3n:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(HITS)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Primero compilamos la colecci\u00f3n de p\u00e1ginas como se mencion\u00f3 anteriormente. Luego, inicializamos las puntuaciones de autoridad y centro para cada p\u00e1gina y finalmente actualizamos y normalizamos los valores hasta la convergencia.\n", "\n", "Una descripci\u00f3n general r\u00e1pida de las funciones auxiliares que utilizamos:\n", "\n", "* `relevant_pages`: Devuelve p\u00e1ginas relevantes de `pagesIndex` dada una consulta.\n", "\n", "* `expand_pages`: Agrega a la colecci\u00f3n p\u00e1ginas vinculadas hacia y desde las `p\u00e1ginas` dadas.\n", "\n", "* `normalizar`: normaliza las puntuaciones de autoridad y centro.\n", "\n", "* `ConvergenceDetector`: una clase que verifica la convergencia, manteniendo un historial de las puntuaciones de las p\u00e1ginas y verificando si cambian o no.\n", "\n", "* `P\u00e1gina`: La plantilla para p\u00e1ginas. Almacena la direcci\u00f3n, puntuaciones de autoridad/centro y enlaces de entrada/salida."]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["### Ejemplo\n", "\n", "Antes de comenzar, necesitamos definir una lista de p\u00e1ginas de muestra en las que trabajar. Las p\u00e1ginas son `pA`, `pB`, etc. y su texto viene dado por `testHTML` y `testHTML2`. La clase `Page` toma como argumentos los enlaces de entrada y salida como listas. Para la p\u00e1gina \"A\", los enlaces de entrada son \"B\", \"C\" y \"E\", mientras que el \u00fanico enlace de salida es \"D\".\n", "\n", "Tambi\u00e9n necesitamos configurar las variables globales `nlp` `pageDict`, `pagesIndex` y `pagesContent`."]}, {"cell_type": "code", "execution_count": 12, "metadata": {"collapsed": true}, "outputs": [], "source": ["testHTML = \"\"\"Like most other male mammals, a man inherits an\n", "            X from his mom and a Y from his dad.\"\"\"\n", "testHTML2 = \"a mom and a dad\"\n", "\n", "pA = Page('A', ['B', 'C', 'E'], ['D'])\n", "pB = Page('B', ['E'], ['A', 'C', 'D'])\n", "pC = Page('C', ['B', 'E'], ['A', 'D'])\n", "pD = Page('D', ['A', 'B', 'C', 'E'], [])\n", "pE = Page('E', [], ['A', 'B', 'C', 'D', 'F'])\n", "pF = Page('F', ['E'], [])\n", "\n", "nlp.pageDict = {pA.address: pA, pB.address: pB, pC.address: pC,\n", "                pD.address: pD, pE.address: pE, pF.address: pF}\n", "\n", "nlp.pagesIndex = nlp.pageDict\n", "\n", "nlp.pagesContent ={pA.address: testHTML, pB.address: testHTML2,\n", "                   pC.address: testHTML, pD.address: testHTML2,\n", "                   pE.address: testHTML, pF.address: testHTML2}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora podemos ejecutar el algoritmo HITS. Nuestra consulta ser\u00e1 'mam\u00edferos' (tenga en cuenta que si bien el contenido del HTML no importa, debe incluir las palabras de consulta o de lo contrario no se seleccionar\u00e1 ninguna p\u00e1gina en el primer paso)."]}, {"cell_type": "code", "execution_count": 13, "metadata": {"collapsed": true}, "outputs": [], "source": ["HITS('mammals')\n", "page_list = ['A', 'B', 'C', 'D', 'E', 'F']\n", "auth_list = [pA.authority, pB.authority, pC.authority, pD.authority, pE.authority, pF.authority]\n", "hub_list = [pA.hub, pB.hub, pC.hub, pD.hub, pE.hub, pF.hub]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Veamos c\u00f3mo se puntuaron las p\u00e1ginas:"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["A: total=0.7696163397038682, auth=0.5583254178509696, hub=0.2112909218528986\n", "B: total=0.7795962360479536, auth=0.23657856688600404, hub=0.5430176691619495\n", "C: total=0.8204496913590655, auth=0.4211098490570872, hub=0.3993398423019784\n", "D: total=0.6316647735856309, auth=0.6316647735856309, hub=0.0\n", "E: total=0.7078245882072104, auth=0.0, hub=0.7078245882072104\n", "F: total=0.23657856688600404, auth=0.23657856688600404, hub=0.0\n"]}], "source": ["for i in range(6):\n", "    p = page_list[i]\n", "    a = auth_list[i]\n", "    h = hub_list[i]\n", "    \n", "    print(\"{}: total={}, auth={}, hub={}\".format(p, a + h, a, h))"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["La puntuaci\u00f3n m\u00e1xima es 0,82 por \"C\". Esta es la p\u00e1gina m\u00e1s relevante seg\u00fan el algoritmo. Puede ver que las p\u00e1ginas a las que enlaza, \"A\" y \"D\", tienen las dos puntuaciones de autoridad m\u00e1s altas (por lo tanto, \"C\" tiene una puntuaci\u00f3n central alta) y las p\u00e1ginas desde las que enlaza, \"B\" y \"E\". , tienen las puntuaciones centrales m\u00e1s altas (por lo que \"C\" tiene una puntuaci\u00f3n de autoridad alta). Combinando estos dos hechos, obtenemos que \"C\" es la p\u00e1gina m\u00e1s relevante. Vale la pena se\u00f1alar que no importa si la p\u00e1gina dada contiene las palabras de consulta, solo que enlace y est\u00e9 enlazada desde p\u00e1ginas de alta calidad."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## RESPUESTA A PREGUNTAS\n", "\n", "**Respuesta de preguntas** es un tipo de sistema de recuperaci\u00f3n de informaci\u00f3n, donde tenemos una pregunta en lugar de una consulta y en lugar de documentos relevantes queremos que la computadora devuelva una oraci\u00f3n, frase o palabra corta que responda a nuestra pregunta. Para comprender mejor el concepto de sistemas de respuesta a preguntas, primero puede leer la secci\u00f3n \"Modelos de texto\" y \"Recuperaci\u00f3n de informaci\u00f3n\" de [text notebook](https://github.com/aimacode/aima-python/blob/master/text.ipynb).\n", "\n", "Un ejemplo t\u00edpico de tal sistema es \"AskMSR\" (Banko *et al.*, 2002), un sistema para responder preguntas que funcion\u00f3 admirablemente frente a algoritmos m\u00e1s sofisticados. La idea b\u00e1sica detr\u00e1s de esto es que muchas preguntas ya han sido respondidas en la web en numerosas ocasiones. El sistema no sabe mucho sobre verbos, conceptos o incluso qu\u00e9 es un sustantivo. Conoce alrededor de 15 tipos diferentes de preguntas y c\u00f3mo se pueden escribir como consultas. Puede reescribir [\u00bfQui\u00e9n era el segundo al mando de George Washington?] como la consulta [\\* era el segundo al mando de George Washington] o [el segundo al mando de George Washington era \\*].\n", "\n", "Despu\u00e9s de reescribir las preguntas, emite estas consultas y recupera el texto breve sobre los t\u00e9rminos de la consulta. Luego divide el resultado en 1, 2 o 3 gramos. Tambi\u00e9n se aplican filtros para aumentar las posibilidades de una respuesta correcta. Si la consulta comienza con \"qui\u00e9n\", filtramos por nombres, si comienza con \"cu\u00e1ntos\", filtramos por n\u00fameros, etc. Tambi\u00e9n podemos filtrar las palabras que aparecen en la consulta. Para la consulta anterior, la respuesta \"George Washington\" es incorrecta, aunque es muy posible que los 2 gramos aparezcan mucho en los t\u00e9rminos de la consulta.\n", "\n", "Finalmente, los diferentes resultados se ponderan por la generalidad de las consultas. El resultado de la consulta booleana general [George Washington O segundo al mando] pesa menos que la consulta m\u00e1s espec\u00edfica [el segundo al mando de George Washington era \\*]. Como respuesta, devolvemos el ngrama mejor clasificado."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## CICLO DE AN\u00c1LISIS\n", "\n", "### Descripci\u00f3n general\n", "\n", "El an\u00e1lisis sint\u00e1ctico (o **an\u00e1lisis**) de una oraci\u00f3n es el proceso de descubrir la estructura sint\u00e1ctica de la oraci\u00f3n de acuerdo con las reglas de una gram\u00e1tica. Hay dos enfoques principales para el an\u00e1lisis. *De arriba hacia abajo*, comenzamos con el s\u00edmbolo inicial y construimos un \u00e1rbol de an\u00e1lisis con las palabras dadas como hojas, y *de abajo hacia arriba*, donde comenzamos a partir de las palabras dadas y construimos un \u00e1rbol que tiene el s\u00edmbolo inicial como ra\u00edz . Ambos enfoques implican \"adivinar\" con anticipaci\u00f3n, por lo que es muy posible que lleve mucho tiempo analizar una oraci\u00f3n (una suposici\u00f3n incorrecta significa mucho retroceso). Afortunadamente, se dedica mucho esfuerzo a analizar subcadenas ya analizadas, por lo que podemos seguir un enfoque de programaci\u00f3n din\u00e1mica para almacenar y reutilizar estos an\u00e1lisis en lugar de recalcularlos. El *Algoritmo de an\u00e1lisis CYK* (llamado as\u00ed por sus inventores, Cocke, Younger y Kasami) utiliza esta t\u00e9cnica para analizar oraciones de una gram\u00e1tica en *Forma normal de Chomsky*.\n", "\n", "El algoritmo CYK devuelve una matriz *M x N x N* (llamada *P*), donde *N* es el n\u00famero de palabras de la oraci\u00f3n y *M* el n\u00famero de s\u00edmbolos no terminales de la gram\u00e1tica. Cada elemento de esta matriz muestra la probabilidad de que una subcadena se transforme a partir de un no terminal en particular. Para encontrar el an\u00e1lisis m\u00e1s probable de la oraci\u00f3n, se requiere una b\u00fasqueda en la matriz resultante. Los algoritmos heur\u00edsticos de b\u00fasqueda funcionan bien en este espacio y podemos derivar las heur\u00edsticas de las propiedades de la gram\u00e1tica.\n", "\n", "En resumen, el algoritmo funciona as\u00ed: hay un bucle externo que determina la longitud de la subcadena. Luego, el algoritmo recorre las palabras de la oraci\u00f3n. Para cada palabra, recorre nuevamente todas las palabras a su derecha hasta la longitud del primer bucle. La subcadena con la que trabajar\u00e1 en esta iteraci\u00f3n son las palabras de la palabra del segundo ciclo con la longitud del primer ciclo. Finalmente, recorre todas las reglas de la gram\u00e1tica y actualiza la probabilidad de la subcadena para cada no terminal del lado derecho."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implementaci\u00f3n\n", "\n", "La implementaci\u00f3n toma como entrada una lista de palabras y una gram\u00e1tica probabil\u00edstica (de la clase `ProbGrammar` detallada anteriormente) en CNF y devuelve la tabla/diccionario *P*. La clave de un elemento en *P* es una tupla en la forma `(No terminal, inicio de subcadena, longitud de subcadena)` y el valor es una probabilidad. Por ejemplo, para la oraci\u00f3n \"el mono est\u00e1 bailando\" y la subcadena \"el mono\" un elemento puede ser `('NP', 0, 2): 0.5`, que significa las dos primeras palabras (la subcadena del \u00edndice 0 y longitud 2) tienen una probabilidad de 0,5 de provenir del terminal `NP`.\n", "\n", "Antes de continuar, puedes echar un vistazo al c\u00f3digo fuente ejecutando la siguiente celda:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(CYK_parse)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Al actualizar la probabilidad de una subcadena, elegimos el m\u00e1ximo de la actual y la probabilidad de que la subcadena se divida en dos partes: una de la palabra del segundo bucle con longitud del tercer bucle y la otra desde el final de la primera parte hasta el final. resto de la longitud del primer bucle."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Ejemplo\n", "\n", "Construyamos una gram\u00e1tica probabil\u00edstica en CNF:"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"collapsed": true}, "outputs": [], "source": ["E_Prob_Chomsky = ProbGrammar(\"E_Prob_Chomsky\", # A Probabilistic Grammar in CNF\n", "                             ProbRules(\n", "                                S = \"NP VP [1]\",\n", "                                NP = \"Article Noun [0.6] | Adjective Noun [0.4]\",\n", "                                VP = \"Verb NP [0.5] | Verb Adjective [0.5]\",\n", "                             ),\n", "                             ProbLexicon(\n", "                                Article = \"the [0.5] | a [0.25] | an [0.25]\",\n", "                                Noun = \"robot [0.4] | sheep [0.4] | fence [0.2]\",\n", "                                Adjective = \"good [0.5] | new [0.2] | sad [0.3]\",\n", "                                Verb = \"is [0.5] | say [0.3] | are [0.2]\"\n", "                             ))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora veamos la tabla de probabilidades de la frase \"el robot es bueno\":"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["defaultdict(<class 'float'>, {('Adjective', 1, 1): 0.0, ('NP', 0, 3): 0.0, ('Verb', 1, 1): 0.0, ('NP', 0, 2): 0.12, ('S', 1, 2): 0.0, ('Article', 2, 1): 0.0, ('NP', 3, 1): 0.0, ('S', 1, 3): 0.0, ('Adjective', 1, 3): 0.0, ('VP', 0, 4): 0.0, ('Article', 0, 3): 0.0, ('Adjective', 1, 2): 0.0, ('Verb', 1, 2): 0.0, ('Adjective', 0, 2): 0.0, ('Article', 0, 1): 0.5, ('VP', 1, 1): 0.0, ('Verb', 0, 2): 0.0, ('Adjective', 0, 3): 0.0, ('VP', 1, 2): 0.0, ('Verb', 0, 3): 0.0, ('NP', 2, 2): 0.0, ('S', 2, 2): 0.0, ('NP', 1, 3): 0.0, ('VP', 1, 3): 0.0, ('Adjective', 3, 1): 0.5, ('Adjective', 0, 1): 0.0, ('NP', 1, 2): 0.0, ('Verb', 0, 1): 0.0, ('S', 0, 3): 0.0, ('NP', 1, 1): 0.0, ('NP', 2, 1): 0.0, ('S', 0, 2): 0.0, ('Noun', 1, 2): 0.0, ('S', 0, 4): 0.015, ('Noun', 1, 3): 0.0, ('Noun', 3, 1): 0.0, ('Noun', 2, 2): 0.0, ('NP', 0, 4): 0.0, ('VP', 2, 2): 0.125, ('Noun', 2, 1): 0.0, ('Noun', 1, 1): 0.4, ('VP', 0, 3): 0.0, ('Article', 1, 2): 0.0, ('Article', 1, 1): 0.0, ('VP', 2, 1): 0.0, ('Adjective', 2, 1): 0.0, ('Verb', 2, 1): 0.5, ('Adjective', 2, 2): 0.0, ('VP', 3, 1): 0.0, ('NP', 0, 1): 0.0, ('VP', 0, 2): 0.0, ('Article', 0, 2): 0.0})\n"]}], "source": ["words = ['the', 'robot', 'is', 'good']\n", "grammar = E_Prob_Chomsky\n", "\n", "P = CYK_parse(words, grammar)\n", "print(P)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Se devuelve un objeto `defaultdict` (`defaultdict` es b\u00e1sicamente un diccionario pero con un valor/tipo predeterminado). Las claves son tuplas en la forma mencionada anteriormente y los valores son las probabilidades correspondientes. La mayor\u00eda de los elementos/an\u00e1lisis tienen una probabilidad de 0. Filtr\u00e9moslos para ver mejor los an\u00e1lisis que importan."]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{('Noun', 1, 1): 0.4, ('VP', 2, 2): 0.125, ('Adjective', 3, 1): 0.5, ('S', 0, 4): 0.015, ('Article', 0, 1): 0.5, ('NP', 0, 2): 0.12, ('Verb', 2, 1): 0.5}\n"]}], "source": ["parses = {k: p for k, p in P.items() if p >0}\n", "\n", "print(parses)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El elemento `('Art\u00edculo', 0, 1): 0,5` significa que el primer elemento proviene del no terminal `Art\u00edculo` con una probabilidad de 0,5. Un elemento m\u00e1s complicado, uno con dos palabras, es `('NP', 0, 2): 0.12` que cubre las dos primeras palabras. La probabilidad de que la subcadena \"el robot\" provenga del no terminal \"NP\" es 0,12. Intentemos seguir las transformaciones de `NP` a las palabras dadas (de arriba hacia abajo) para asegurarnos de que este sea realmente el caso:\n", "\n", "1. La probabilidad de que \"NP\" se transforme en \"Sustantivo de art\u00edculo\" es 0,6.\n", "\n", "2. La probabilidad de que \"Art\u00edculo\" se transforme en \"el\" es 0,5 (probabilidad total = 0,6*0,5 = 0,3).\n", "\n", "3. La probabilidad de que \"Sustantivo\" se transforme en \"robot\" es 0,4 (total = 0,3*0,4 = 0,12).\n", "\n", "Por tanto, la probabilidad total de la transformaci\u00f3n es 0,12.\n", "\n", "Observe c\u00f3mo la probabilidad para toda la cadena (dada por la clave `('S', 0, 4)`) es 0,015. Esto significa que el an\u00e1lisis m\u00e1s probable de la oraci\u00f3n tiene una probabilidad de 0,015."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## AN\u00c1LISIS DE GR\u00c1FICOS\n", "\n", "### Descripci\u00f3n general\n", "\n", "Veamos ahora un algoritmo de an\u00e1lisis de gr\u00e1ficos m\u00e1s general. Dadas una gram\u00e1tica no probabil\u00edstica y una oraci\u00f3n, este algoritmo construye un \u00e1rbol de an\u00e1lisis de arriba hacia abajo, con las palabras de la oraci\u00f3n como hojas. Funciona con un enfoque de programaci\u00f3n din\u00e1mica, creando un gr\u00e1fico para almacenar an\u00e1lisis de subcadenas para que no tenga que analizarlas nuevamente (al igual que el algoritmo CYK). Cada no terminal, comenzando desde S, es reemplazado por sus reglas del lado derecho en el gr\u00e1fico, hasta que terminemos con los an\u00e1lisis correctos.\n", "\n", "### Implementaci\u00f3n\n", "\n", "Un an\u00e1lisis tiene la forma `[inicio, fin, no terminal, sub\u00e1rbol, transformaci\u00f3n esperada]`, donde `sub\u00e1rbol` es un \u00e1rbol con el correspondiente `no terminal` como ra\u00edz y `esperado- La transformaci\u00f3n \"es una regla del lado derecho del\" no terminal \".\n", "\n", "El an\u00e1lisis del gr\u00e1fico se implementa en una clase, \"Gr\u00e1fico\". Se inicializa con una gram\u00e1tica y puede devolver la lista de todos los an\u00e1lisis de una oraci\u00f3n con la funci\u00f3n \"an\u00e1lisis\".\n", "\n", "El gr\u00e1fico es una lista de listas. Las listas corresponden a las longitudes de las subcadenas (incluida la cadena vac\u00eda), de principio a fin. Cuando decimos \"un punto en el gr\u00e1fico\", nos referimos a una lista de cierta longitud.\n", "\n", "Un resumen r\u00e1pido de las funciones de clase:"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["* `parses`: Devuelve una lista de an\u00e1lisis para una oraci\u00f3n determinada. Si la oraci\u00f3n no se puede analizar, devolver\u00e1 una lista vac\u00eda. Inicializa el proceso llamando a \"parse\" desde el s\u00edmbolo inicial.\n", "\n", "\n", "* `parse`: analiza la lista de palabras y crea el gr\u00e1fico.\n", "\n", "\n", "* `add_edge`: Agrega otro borde al gr\u00e1fico en un punto determinado. Adem\u00e1s, examina si el borde se extiende o predice otro borde. Si el borde en s\u00ed no espera una transformaci\u00f3n, extender\u00e1 otros bordes y, de lo contrario, predecir\u00e1 los bordes.\n", "\n", "\n", "* `esc\u00e1ner`: Dada una palabra y un punto en el gr\u00e1fico, extiende los bordes que esperaban una transformaci\u00f3n que puede resultar en la palabra dada. Por ejemplo, si la palabra 'el' es un 'Art\u00edculo' y estamos examinando dos bordes en el punto de un gr\u00e1fico, uno esperando un 'Art\u00edculo' y el otro un 'Verbo', el primero se extender\u00e1 mientras que el segundo no lo har\u00e1.\n", "\n", "\n", "* `predictor`: si un borde no puede extender otros bordes (porque est\u00e1 esperando una transformaci\u00f3n en s\u00ed), agregaremos al gr\u00e1fico reglas/transformaciones que pueden ayudar a extender el borde. Las nuevas aristas provienen del lado derecho de las reglas de transformaci\u00f3n esperadas. Por ejemplo, si un borde espera la transformaci\u00f3n 'Adjetivo Sustantivo', agregaremos al gr\u00e1fico un borde para cada regla del lado derecho del 'Adjetivo' no terminal.\n", "\n", "\n", "* `extender`: Extiende los bordes dado un borde (llamado `E`). Si el no terminal de `E` es la misma que la transformaci\u00f3n esperada de otra arista (llam\u00e9mosla `A`), agregue al gr\u00e1fico una nueva arista con el no terminal de `A` y las transformaciones de `A `menos el no terminal que coincidi\u00f3 con el no terminal de `E`. Por ejemplo, si una arista \"E\" tiene \"Art\u00edculo\" como no terminal y no espera ninguna transformaci\u00f3n, necesitamos ver qu\u00e9 aristas puede extender. Examinemos el borde `N`. Esto espera una transformaci\u00f3n de 'Noun Verb'. 'Sustantivo' no coincide con 'Art\u00edculo', as\u00ed que seguimos adelante. Otra arista, \"A\", espera una transformaci\u00f3n de \"Article Noun\" y tiene un no terminal de \"NP\". \u00a1Tenemos un partido! Se agregar\u00e1 una nueva arista con 'NP' como su no terminal (el no terminal de 'A') y 'Sustantivo' como la transformaci\u00f3n esperada (el resto de la transformaci\u00f3n esperada de 'A')."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Puede ver el c\u00f3digo fuente ejecutando la siguiente celda:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["psource(Chart)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Ejemplo\n", "\n", "Usaremos la gram\u00e1tica `E0` para analizar la oraci\u00f3n \"el hedor est\u00e1 en 2 2\".\n", "\n", "Primero necesitamos construir un objeto \"Gr\u00e1fico\":"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"collapsed": true}, "outputs": [], "source": ["chart = Chart(nlp.E0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y luego simplemente llamamos a la funci\u00f3n `parses`:"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[[0, 6, 'S', [[0, 2, 'NP', [('Article', 'the'), ('Noun', 'stench')], []], [2, 6, 'VP', [[2, 3, 'VP', [('Verb', 'is')], []], [3, 6, 'PP', [('Preposition', 'in'), [4, 6, 'NP', [('Digit', '2'), ('Digit', '2')], []]], []]], []]], []]]\n"]}], "source": ["print(chart.parses('the stench is in 2 2'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Puede ver qu\u00e9 bordes se agregan estableciendo el argumento de inicializaci\u00f3n opcional \"trace\" en verdadero."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chart_trace = Chart(nlp.E0, trace=True)\n", "chart_trace.parses('the stench is in 2 2')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Intentemos analizar una oraci\u00f3n que la gram\u00e1tica no reconoce:"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[]\n"]}], "source": ["print(chart.parses('the stench 2 2'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Se devolvi\u00f3 una lista vac\u00eda."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.2"}}, "nbformat": 4, "nbformat_minor": 1}